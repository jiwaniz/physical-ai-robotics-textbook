# Week 11: Vision-Language-Action Ø¨Ù†ÛŒØ§Ø¯ÛŒÚº

## Ø®Ù„Ø§ØµÛ

Ø­ØªÙ…ÛŒ Ù…Ø§ÚˆÛŒÙˆÙ„ Ù…ÛŒÚº Ø®ÙˆØ´ Ø¢Ù…Ø¯ÛŒØ¯! ÛŒÛ ÛÙØªÛ Vision-Language-Action (VLA) Ù…Ø§ÚˆÙ„Ø² Ù…ØªØ¹Ø§Ø±Ù Ú©Ø±Ø§ØªØ§ ÛÛ’ØŒ Ø¬Ùˆ Physical AI Ú©ÛŒ Ø¬Ø¯ÛŒØ¯ ØªØ±ÛŒÙ† ØµÙˆØ±Øª ÛÛ’Û” Ø¢Ù¾ Ø³ÛŒÚ©Ú¾ÛŒÚº Ú¯Û’ Ú©Û Ú©ÛŒØ³Û’ Ù…Ù„Ù¹ÛŒ Ù…ÙˆÚˆÙ„ transformers Ø±ÙˆØ¨ÙˆÙ¹Ø³ Ú©Ùˆ Ø¨ØµØ±ÛŒ Ù…Ù†Ø§Ø¸Ø± Ø³Ù…Ø¬Ú¾Ù†Û’ØŒ Ù‚Ø¯Ø±ØªÛŒ Ø²Ø¨Ø§Ù† Ú©Û’ Ø§Ø­Ú©Ø§Ù…Ø§Øª Ú©ÛŒ ØªØ´Ø±ÛŒØ­ Ú©Ø±Ù†Û’ØŒ Ø§ÙˆØ± Ù…Ù†Ø§Ø³Ø¨ actions Ù¾ÛŒØ¯Ø§ Ú©Ø±Ù†Û’ Ú©Û’ Ù‚Ø§Ø¨Ù„ Ø¨Ù†Ø§ØªÛ’ ÛÛŒÚºÛ”

## Ø³ÛŒÚ©Ú¾Ù†Û’ Ú©Û’ Ù…Ù‚Ø§ØµØ¯

Ø§Ø³ ÛÙØªÛ’ Ú©Û’ Ø§Ø®ØªØªØ§Ù… ØªÚ©ØŒ Ø¢Ù¾ ÛŒÛ Ú©Ø± Ø³Ú©ÛŒÚº Ú¯Û’:

- VLA Ù…Ø§ÚˆÙ„ Ø¢Ø±Ú©ÛŒÙ¹ÛŒÚ©Ú†Ø± Ø§ÙˆØ± Ø§Ø³ Ú©Û’ Ø§Ø¬Ø²Ø§Ø¡ Ú©Ùˆ Ø³Ù…Ø¬Ú¾Ù†Ø§
- ÙˆØ¶Ø§Ø­Øª Ú©Ø±Ù†Ø§ Ú©Û visionØŒ languageØŒ Ø§ÙˆØ± action modalities Ú©Ùˆ Ú©ÛŒØ³Û’ ÛŒÚ©Ø¬Ø§ Ú©ÛŒØ§ Ø¬Ø§ØªØ§ ÛÛ’
- Ù¾ÛÙ„Û’ Ø³Û’ ØªØ±Ø¨ÛŒØª ÛŒØ§ÙØªÛ Ù…Ø§ÚˆÙ„Ø² Ú©Û’ Ø³Ø§ØªÚ¾ Ø§ÛŒÚ© Ø³Ø§Ø¯Û VLA pipeline Ù†Ø§ÙØ° Ú©Ø±Ù†Ø§
- Ù…Ø®ØµÙˆØµ Ø±ÙˆØ¨ÙˆÙ¹Ú© Ú©Ø§Ù…ÙˆÚº Ú©Û’ Ù„ÛŒÛ’ VLA Ù…Ø§ÚˆÙ„Ø² Ú©Ùˆ fine-tune Ú©Ø±Ù†Ø§
- VLA Ù…Ø§ÚˆÙ„Ø² Ú©Ùˆ ROS 2 Ø§ÙˆØ± simulators Ú©Û’ Ø³Ø§ØªÚ¾ Ø§Ù†Ù¹ÛŒÚ¯Ø±ÛŒÙ¹ Ú©Ø±Ù†Ø§
- VLA Ù…Ø§ÚˆÙ„ Ú©ÛŒ Ú©Ø§Ø±Ú©Ø±Ø¯Ú¯ÛŒ Ú©Ø§ Ù…Ù‚Ø¯Ø§Ø±ÛŒ Ø·ÙˆØ± Ù¾Ø± Ø¬Ø§Ø¦Ø²Û Ù„ÛŒÙ†Ø§

## VLA Ù…Ø§ÚˆÙ„Ø² Ú©ÛŒØ§ ÛÛŒÚºØŸ

**Vision-Language-Action (VLA)** Ù…Ø§ÚˆÙ„Ø² Ù…Ù„Ù¹ÛŒ Ù…ÙˆÚˆÙ„ transformers ÛÛŒÚº Ø¬Ùˆ ÛŒÛ map Ú©Ø±ØªÛ’ ÛÛŒÚº:
- **Vision** (camera ØªØµØ§ÙˆÛŒØ±ØŒ depthØŒ point clouds)
- **Language** (Ù‚Ø¯Ø±ØªÛŒ Ø²Ø¨Ø§Ù† Ú©ÛŒ ÛØ¯Ø§ÛŒØ§Øª)
â†’ **Actions** (robot motor commandsØŒ trajectories)

### VLA Ù…Ø§ÚˆÙ„Ø² Ú©ÛŒÙˆÚºØŸ

**Ø±ÙˆØ§ÛŒØªÛŒ Ø±ÙˆØ¨ÙˆÙ¹Ú©Ø³ pipeline:**
```
Perception â†’ State Estimation â†’ Planning â†’ Control
(Ø¹Ù„ÛŒØ­Ø¯Û modulesØŒ ÛØ§ØªÚ¾ Ø³Û’ Ø¨Ù†Ø§Ø¦Û’ Ú¯Ø¦Û’ interfaces)
```

**VLA approach:**
```
Vision + Language â†’ Transformer â†’ Actions
(end-to-end learningØŒ unified representation)
```

**ÙÙˆØ§Ø¦Ø¯:**
- **Ø¹Ù…ÙˆÙ…ÛŒØª**: ÙˆØ§Ø­Ø¯ Ù…Ø§ÚˆÙ„ Ù…ØªÙ†ÙˆØ¹ Ú©Ø§Ù…ÙˆÚº Ú©Ùˆ Ø³Ù†Ø¨Ú¾Ø§Ù„ØªØ§ ÛÛ’
- **Ù‚Ø¯Ø±ØªÛŒ interaction**: Ø²Ø¨Ø§Ù† Ú©ÛŒ ÛØ¯Ø§ÛŒØ§Øª Ù‚Ø¨ÙˆÙ„ Ú©Ø±ØªØ§ ÛÛ’
- **Transfer learning**: Ø§Ù†Ù¹Ø±Ù†ÛŒÙ¹ Ø³Ú©ÛŒÙ„ vision-language ÚˆÛŒÙ¹Ø§ Ø³Û’ ÙØ§Ø¦Ø¯Û Ø§Ù¹Ú¾Ø§ØªØ§ ÛÛ’
- **Ø³Ø§Ø¯Ú¯ÛŒ**: ÛØ§ØªÚ¾ Ø³Û’ Ø¨Ù†Ø§Ø¦Û’ Ú¯Ø¦Û’ perception/planning modules Ú©ÛŒ Ø¶Ø±ÙˆØ±Øª Ù†ÛÛŒÚº

### Ø­Ù‚ÛŒÙ‚ÛŒ Ø¯Ù†ÛŒØ§ Ú©Û’ VLA Ø³Ø³Ù¹Ù…Ø²

| Model | Organization | Key Achievement |
|-------|-------------|-----------------|
| **RT-1** | Google DeepMind | Ø­Ù‚ÛŒÙ‚ÛŒ Ø±ÙˆØ¨ÙˆÙ¹Ø³ Ù¾Ø± 700 Ú©Ø§Ù… |
| **RT-2** | Google DeepMind | Vision-language model â†’ actions |
| **PaLM-E** | Google | 540B parameter embodied AI |
| **RoboFlamingo** | Open source | Open-weights VLA |
| **OpenVLA** | Open source | 7B parameter open model |

## VLA Ø¢Ø±Ú©ÛŒÙ¹ÛŒÚ©Ú†Ø±

### Ø§Ø¹Ù„ÛŒÙ° Ø³Ø·Ø­ÛŒ ÚˆÚ¾Ø§Ù†Ú†Û

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Camera    â”‚â”€â”€â”€â”€â–¶â”‚   Vision    â”‚
â”‚   Image     â”‚     â”‚   Encoder   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜
                           â”‚
                           â–¼
                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”‚             â”‚     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Language   â”‚â”€â”€â”€â”€â–¶â”‚ Transformer â”‚â”€â”€â”€â”€â–¶â”‚   Action    â”‚
â”‚ Instruction â”‚     â”‚    Core     â”‚     â”‚   Decoder   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â”‚             â”‚     â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜
                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜            â”‚
                                               â–¼
                                        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                                        â”‚   Robot     â”‚
                                        â”‚   Actions   â”‚
                                        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Ø¬Ø²Ùˆ Ú©ÛŒ ØªÙØµÛŒÙ„Ø§Øª

#### 1. Vision Encoder

ØªØµØ§ÙˆÛŒØ± Ú©Ùˆ token embeddings Ù…ÛŒÚº ØªØ¨Ø¯ÛŒÙ„ Ú©Ø±ØªØ§ ÛÛ’:

```python
from transformers import CLIPVisionModel, CLIPProcessor

class VisionEncoder:
    def __init__(self):
        # Use CLIP vision encoder (pre-trained on image-text pairs)
        self.model = CLIPVisionModel.from_pretrained("openai/clip-vit-base-patch32")
        self.processor = CLIPProcessor.from_pretrained("openai/clip-vit-base-patch32")

    def encode(self, image):
        """
        Args:
            image: PIL Image or numpy array (H, W, 3)
        Returns:
            vision_tokens: (N_patches, hidden_dim)
        """
        # Preprocess image
        inputs = self.processor(images=image, return_tensors="pt")

        # Extract features
        outputs = self.model(**inputs)
        vision_tokens = outputs.last_hidden_state  # (1, N_patches, 768)

        return vision_tokens.squeeze(0)  # (N_patches, 768)
```

**Ú©Ù„ÛŒØ¯ÛŒ architectures:**
- **ViT (Vision Transformer)**: Patch-based image encoding
- **CLIP**: Vision-language contrastive learning
- **DinoV2**: Self-supervised vision features
- **R3M**: Robot-specific vision representations

#### 2. Language Encoder

Ù…ØªÙ† Ú©ÛŒ ÛØ¯Ø§ÛŒØ§Øª Ú©Ùˆ encode Ú©Ø±ØªØ§ ÛÛ’:

```python
from transformers import T5Tokenizer, T5EncoderModel

class LanguageEncoder:
    def __init__(self):
        self.tokenizer = T5Tokenizer.from_pretrained("google/flan-t5-base")
        self.model = T5EncoderModel.from_pretrained("google/flan-t5-base")

    def encode(self, instruction):
        """
        Args:
            instruction: str, e.g., "pick up the red cube"
        Returns:
            language_tokens: (seq_len, hidden_dim)
        """
        # Tokenize
        inputs = self.tokenizer(instruction, return_tensors="pt")

        # Encode
        outputs = self.model(**inputs)
        language_tokens = outputs.last_hidden_state  # (1, seq_len, 768)

        return language_tokens.squeeze(0)  # (seq_len, 768)
```

**Ø¹Ø§Ù… encoders:**
- **T5**: Text-to-text transformer
- **BERT**: Bidirectional language model
- **LLaMA/GPT**: Large language models
- **CLIP Text**: Vision Ú©Û’ Ø³Ø§ØªÚ¾ aligned

#### 3. Transformer Core

Vision Ø§ÙˆØ± language Ú©Ùˆ fuse Ú©Ø±ØªØ§ ÛÛ’ØŒ action representations output Ú©Ø±ØªØ§ ÛÛ’:

```python
import torch
import torch.nn as nn

class VLATransformer(nn.Module):
    def __init__(self, hidden_dim=768, num_layers=6, num_heads=12):
        super().__init__()

        # Multimodal transformer
        encoder_layer = nn.TransformerEncoderLayer(
            d_model=hidden_dim,
            nhead=num_heads,
            dim_feedforward=hidden_dim * 4,
            batch_first=True
        )
        self.transformer = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)

        # Modality embeddings (distinguish vision vs language tokens)
        self.vision_embedding = nn.Parameter(torch.randn(1, 1, hidden_dim))
        self.language_embedding = nn.Parameter(torch.randn(1, 1, hidden_dim))

    def forward(self, vision_tokens, language_tokens):
        """
        Args:
            vision_tokens: (batch, N_patches, hidden_dim)
            language_tokens: (batch, seq_len, hidden_dim)
        Returns:
            fused_features: (batch, N_patches + seq_len, hidden_dim)
        """
        batch_size = vision_tokens.shape[0]

        # Add modality embeddings
        vision_tokens = vision_tokens + self.vision_embedding
        language_tokens = language_tokens + self.language_embedding

        # Concatenate vision and language tokens
        combined_tokens = torch.cat([vision_tokens, language_tokens], dim=1)

        # Transform
        fused_features = self.transformer(combined_tokens)

        return fused_features
```

#### 4. Action Decoder

Fused features Ø³Û’ robot actions Ú©ÛŒ Ù¾ÛŒØ´ÛŒÙ† Ú¯ÙˆØ¦ÛŒ Ú©Ø±ØªØ§ ÛÛ’:

```python
class ActionDecoder(nn.Module):
    def __init__(self, hidden_dim=768, action_dim=7, action_horizon=10):
        super().__init__()

        self.action_dim = action_dim
        self.action_horizon = action_horizon  # Predict multiple timesteps

        # Action query tokens (learnable)
        self.action_queries = nn.Parameter(
            torch.randn(1, action_horizon, hidden_dim)
        )

        # Cross-attention: action queries attend to fused features
        self.cross_attention = nn.MultiheadAttention(
            embed_dim=hidden_dim,
            num_heads=12,
            batch_first=True
        )

        # Action head
        self.action_head = nn.Linear(hidden_dim, action_dim)

    def forward(self, fused_features):
        """
        Args:
            fused_features: (batch, seq_len, hidden_dim)
        Returns:
            actions: (batch, action_horizon, action_dim)
        """
        batch_size = fused_features.shape[0]

        # Expand action queries for batch
        action_queries = self.action_queries.expand(batch_size, -1, -1)

        # Cross-attend to fused features
        action_features, _ = self.cross_attention(
            query=action_queries,
            key=fused_features,
            value=fused_features
        )

        # Predict actions
        actions = self.action_head(action_features)  # (batch, horizon, action_dim)

        return actions
```

### Ù…Ú©Ù…Ù„ VLA Ù…Ø§ÚˆÙ„

```python
class VLAModel(nn.Module):
    """Complete Vision-Language-Action model."""

    def __init__(self, action_dim=7, action_horizon=10):
        super().__init__()

        self.vision_encoder = VisionEncoder()
        self.language_encoder = LanguageEncoder()
        self.transformer = VLATransformer(hidden_dim=768)
        self.action_decoder = ActionDecoder(hidden_dim=768, action_dim=action_dim, action_horizon=action_horizon)

    def forward(self, image, instruction):
        """
        Args:
            image: (batch, H, W, 3) or PIL Image
            instruction: str or List[str]
        Returns:
            actions: (batch, action_horizon, action_dim)
        """
        # Encode vision
        vision_tokens = self.vision_encoder.encode(image)  # (N_patches, 768)
        vision_tokens = vision_tokens.unsqueeze(0)  # (1, N_patches, 768)

        # Encode language
        language_tokens = self.language_encoder.encode(instruction)  # (seq_len, 768)
        language_tokens = language_tokens.unsqueeze(0)  # (1, seq_len, 768)

        # Fuse modalities
        fused_features = self.transformer(vision_tokens, language_tokens)

        # Decode actions
        actions = self.action_decoder(fused_features)

        return actions
```

## VLA Ù…Ø§ÚˆÙ„Ø² Ú©ÛŒ ØªØ±Ø¨ÛŒØª

### ÚˆÛŒÙ¹Ø§Ø³ÛŒÙ¹ Ú©Û’ ØªÙ‚Ø§Ø¶Û’

**ÚˆÛŒÙ¹Ø§ Ú©ÛŒ Ø´Ú©Ù„:**
```python
{
    "image": PIL.Image (H, W, 3),
    "instruction": str,
    "actions": np.array (action_horizon, action_dim),
    "success": bool
}
```

**Ù…Ø«Ø§Ù„ Ú©Ø§ trajectory:**
```python
trajectory = {
    "task": "pick up the red block",
    "frames": [
        {
            "image": image_0,  # 640x480 RGB
            "instruction": "pick up the red block",
            "action": [0.1, 0.0, -0.05, 0.0, 0.0, 0.0, 0.0],  # (x, y, z, roll, pitch, yaw, gripper)
            "reward": 0.0
        },
        {
            "image": image_1,
            "instruction": "pick up the red block",
            "action": [0.15, 0.0, -0.1, 0.0, 0.0, 0.0, 0.0],
            "reward": 0.0
        },
        # ... more frames
        {
            "image": image_final,
            "instruction": "pick up the red block",
            "action": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0],  # Close gripper
            "reward": 1.0  # Success!
        }
    ]
}
```

### ØªØ±Ø¨ÛŒØªÛŒ Loop

```python
import torch
from torch.optim import AdamW
from torch.utils.data import DataLoader

def train_vla(model, dataloader, num_epochs=10):
    """Train VLA model with behavior cloning."""

    optimizer = AdamW(model.parameters(), lr=1e-4)
    criterion = nn.MSELoss()  # Action prediction loss

    model.train()

    for epoch in range(num_epochs):
        total_loss = 0.0

        for batch in dataloader:
            images = batch["image"]  # (batch, H, W, 3)
            instructions = batch["instruction"]  # List[str]
            target_actions = batch["actions"]  # (batch, horizon, action_dim)

            # Forward pass
            predicted_actions = model(images, instructions)

            # Compute loss
            loss = criterion(predicted_actions, target_actions)

            # Backward pass
            optimizer.zero_grad()
            loss.backward()
            optimizer.step()

            total_loss += loss.item()

        avg_loss = total_loss / len(dataloader)
        print(f"Epoch {epoch+1}/{num_epochs}, Loss: {avg_loss:.4f}")

    return model
```

### ÚˆÛŒÙ¹Ø§ Ø§Ú©Ù¹Ú¾Ø§ Ú©Ø±Ù†Û’ Ú©ÛŒ Ø­Ú©Ù…Øª Ø¹Ù…Ù„ÛŒ

#### 1. Teleoperation

Ø§Ù†Ø³Ø§Ù† VR/keyboard Ú©Û’ Ø°Ø±ÛŒØ¹Û’ Ú©Ø§Ù…ÙˆÚº Ú©Ø§ Ù…Ø¸Ø§ÛØ±Û Ú©Ø±ØªØ§ ÛÛ’:
```python
def collect_teleop_data(robot, num_episodes=100):
    """Collect data via human teleoperation."""
    dataset = []

    for episode in range(num_episodes):
        # Show random task
        task = sample_task()  # e.g., "pick red cube"
        print(f"Task: {task}")

        # Human controls robot
        trajectory = robot.run_teleoperation(task)

        # Save trajectory
        dataset.append({
            "task": task,
            "trajectory": trajectory
        })

    return dataset
```

#### 2. Simulation (Isaac Sim)

Ø®ÙˆØ¯Ú©Ø§Ø± Ø·ÙˆØ± Ù¾Ø± ÚˆÛŒÙ¹Ø§ Ù¾ÛŒØ¯Ø§ Ú©Ø±ÛŒÚº:
```python
def collect_sim_data(env, policy, num_episodes=10000):
    """Collect data in simulation with domain randomization."""
    dataset = []

    for episode in range(num_episodes):
        obs = env.reset()
        task = env.sample_task()

        trajectory = []
        done = False

        while not done:
            # Use pre-trained policy or scripted behavior
            action = policy(obs, task)

            # Step environment
            next_obs, reward, done, info = env.step(action)

            # Record frame
            trajectory.append({
                "image": obs["image"],
                "instruction": task,
                "action": action
            })

            obs = next_obs

        if info["success"]:
            dataset.append(trajectory)

    return dataset
```

#### 3. Co-Training (Sim + Real)

Simulated Ø§ÙˆØ± Ø­Ù‚ÛŒÙ‚ÛŒ ÚˆÛŒÙ¹Ø§ Ú©Ùˆ Ù…Ù„Ø§Ø¦ÛŒÚº:
```python
# 90% sim, 10% real
dataset = combine_datasets(
    sim_data=collect_sim_data(sim_env, num_episodes=9000),
    real_data=collect_teleop_data(real_robot, num_episodes=1000),
    real_weight=10.0  # Upweight real data
)
```

## Ù¾ÛÙ„Û’ Ø³Û’ ØªØ±Ø¨ÛŒØª ÛŒØ§ÙØªÛ VLA Ù…Ø§ÚˆÙ„Ø² Ú©Ø§ Ø§Ø³ØªØ¹Ù…Ø§Ù„

### OpenVLA (7B Parameters)

```python
from transformers import AutoModel, AutoProcessor

# Load OpenVLA
model = AutoModel.from_pretrained("openvla/openvla-7b")
processor = AutoProcessor.from_pretrained("openvla/openvla-7b")

# Inference
image = load_image("scene.jpg")
instruction = "pick up the blue mug"

inputs = processor(images=image, text=instruction, return_tensors="pt")
outputs = model(**inputs)
actions = outputs.actions  # (1, action_horizon, 7)

# Execute action
robot.execute(actions[0, 0])  # First timestep action
```

### RT-2 (Ø¨Ø°Ø±ÛŒØ¹Û Hugging Face)

```python
# RT-2 is not publicly released, but similar models:
from transformers import RT2Model  # Hypothetical

model = RT2Model.from_pretrained("google/rt-2-base")

# Run inference
action = model.predict(image=image, instruction=instruction)
robot.move_to(action)
```

## ROS 2 Ú©Û’ Ø³Ø§ØªÚ¾ VLA Ú©Ùˆ Ø§Ù†Ù¹ÛŒÚ¯Ø±ÛŒÙ¹ Ú©Ø±Ù†Ø§

```python
#!/usr/bin/env python3
import rclpy
from rclpy.node import Node
from sensor_msgs.msg import Image
from std_msgs.msg import String
from geometry_msgs.msg import Twist
from cv_bridge import CvBridge
import torch

class VLANode(Node):
    """ROS 2 node for VLA model inference."""

    def __init__(self):
        super().__init__('vla_node')

        # Load VLA model
        self.model = VLAModel.from_pretrained("path/to/model")
        self.model.eval()

        # CV bridge
        self.bridge = CvBridge()

        # Subscribers
        self.image_sub = self.create_subscription(
            Image, '/camera/image_raw', self.image_callback, 10
        )
        self.instruction_sub = self.create_subscription(
            String, '/voice/instruction', self.instruction_callback, 10
        )

        # Publisher
        self.action_pub = self.create_publisher(Twist, '/cmd_vel', 10)

        # State
        self.latest_image = None
        self.latest_instruction = None

        # Timer for inference
        self.create_timer(0.1, self.inference_callback)  # 10 Hz

    def image_callback(self, msg):
        """Store latest image."""
        self.latest_image = self.bridge.imgmsg_to_cv2(msg, desired_encoding='rgb8')

    def instruction_callback(self, msg):
        """Store latest instruction."""
        self.latest_instruction = msg.data
        self.get_logger().info(f'Received instruction: {msg.data}')

    def inference_callback(self):
        """Run VLA inference and publish actions."""
        if self.latest_image is None or self.latest_instruction is None:
            return

        # Run VLA model
        with torch.no_grad():
            actions = self.model(self.latest_image, self.latest_instruction)

        # Convert to Twist message (simplified)
        action = actions[0, 0].cpu().numpy()  # First action
        twist = Twist()
        twist.linear.x = float(action[0])
        twist.angular.z = float(action[5])

        # Publish
        self.action_pub.publish(twist)


def main(args=None):
    rclpy.init(args=args)
    node = VLANode()
    rclpy.spin(node)
    node.destroy_node()
    rclpy.shutdown()


if __name__ == '__main__':
    main()
```

## Week 11 Ø¹Ù…Ù„ÛŒ Ù…Ø´Ù‚

**Ú©Ø§Ù…**: Ø±ÙˆØ¨ÙˆÙ¹ manipulation Ú©Û’ Ù„ÛŒÛ’ Ù¾ÛÙ„Û’ Ø³Û’ ØªØ±Ø¨ÛŒØª ÛŒØ§ÙØªÛ vision encoder Ú©Ùˆ fine-tune Ú©Ø±ÛŒÚº

**ØªÙ‚Ø§Ø¶Û’:**
1. RT-1 ÛŒØ§ OpenVLA demo dataset ÚˆØ§Ø¤Ù† Ù„ÙˆÚˆ Ú©Ø±ÛŒÚº (100 trajectories)
2. Ø±ÙˆØ¨ÙˆÙ¹ ØªØµØ§ÙˆÛŒØ± Ù¾Ø± CLIP vision encoder Ú©Ùˆ fine-tune Ú©Ø±ÛŒÚº
3. Ø³Ø§Ø¯Û action prediction head (MLP) Ù†Ø§ÙØ° Ú©Ø±ÛŒÚº
4. 10 epochs Ú©Û’ Ù„ÛŒÛ’ ØªØ±Ø¨ÛŒØª Ø¯ÛŒÚº
5. Action prediction Ú©ÛŒ Ø¯Ø±Ø³ØªÚ¯ÛŒ Ú©Ø§ Ø¬Ø§Ø¦Ø²Û Ù„ÛŒÚº
6. Ù¾ÛŒØ´ÛŒÙ† Ú¯ÙˆØ¦ÛŒ Ø´Ø¯Û Ø¨Ù…Ù‚Ø§Ø¨Ù„Û ground-truth actions Ú©Ùˆ visualize Ú©Ø±ÛŒÚº

**Bonus:**
- Language conditioning Ø´Ø§Ù…Ù„ Ú©Ø±ÛŒÚº
- Isaac Sim environment Ù¾Ø± Ù¹ÛŒØ³Ù¹ Ú©Ø±ÛŒÚº
- Action chunking Ù†Ø§ÙØ° Ú©Ø±ÛŒÚº (10 Ù‚Ø¯Ù… Ø¢Ú¯Û’ Ú©ÛŒ Ù¾ÛŒØ´ÛŒÙ† Ú¯ÙˆØ¦ÛŒ)

## ÙˆØ³Ø§Ø¦Ù„

- [RT-1 Paper](https://arxiv.org/abs/2212.06817) - Robotics Transformer for Real-World Control
- [RT-2 Paper](https://arxiv.org/abs/2307.15818) - Vision-Language-Action Models
- [OpenVLA](https://openvla.github.io/) - Open-source 7B VLA model
- [RoboFlamingo](https://roboflamingo.github.io/) - Open-weights VLA
- [PaLM-E Paper](https://arxiv.org/abs/2303.03378) - Embodied Multimodal Language Model

## Ø§Ú¯Ù„Û’ Ù‚Ø¯Ù…

Ø´Ø§Ù†Ø¯Ø§Ø± Ú©Ø§Ù…! Ø§Ø¨ Ø¢Ù¾ VLA Ø¢Ø±Ú©ÛŒÙ¹ÛŒÚ©Ú†Ø± Ú©Ùˆ Ø³Ù…Ø¬Ú¾ØªÛ’ ÛÛŒÚº Ø§ÙˆØ± Ù¾ÛÙ„Û’ Ø³Û’ ØªØ±Ø¨ÛŒØª ÛŒØ§ÙØªÛ Ù…Ø§ÚˆÙ„Ø² Ú©Û’ Ø³Ø§ØªÚ¾ Ú©Ø§Ù… Ú©Ø± Ø³Ú©ØªÛ’ ÛÛŒÚºÛ”

Ø§Ú¯Ù„Ø§ ÛÙØªÛ: [Week 12: Advanced VLA Applications](week-12.md)

ÛÙ… action chunkingØŒ multi-task learningØŒ Ø§ÙˆØ± Ø­Ù‚ÛŒÙ‚ÛŒ Ø±ÙˆØ¨ÙˆÙ¹Ø³ Ù¾Ø± VLA Ù…Ø§ÚˆÙ„Ø² Ú©ÛŒ ØªØ¹ÛŒÙ†Ø§ØªÛŒ Ú©Ùˆ ØªÙ„Ø§Ø´ Ú©Ø±ÛŒÚº Ú¯Û’!

---

## ğŸ“ ÛÙØªÛ ÙˆØ§Ø± Quiz

Ø§Ø³ ÛÙØªÛ’ Ú©Û’ Ù…ÙˆØ§Ø¯ Ú©ÛŒ Ø§Ù¾Ù†ÛŒ Ø³Ù…Ø¬Ú¾ Ú©Ùˆ Ù¹ÛŒØ³Ù¹ Ú©Ø±ÛŒÚº! Quiz multiple choice ÛÛ’ØŒ Ø®ÙˆØ¯Ú©Ø§Ø± Ø·ÙˆØ± Ù¾Ø± scored ÛÛ’ØŒ Ø§ÙˆØ± Ø¢Ù¾ Ú©Û’ Ù¾Ø§Ø³ 2 attempts ÛÛŒÚºÛ”

**[Week 11 Quiz Ù„ÛŒÚº â†’](/quiz?week=11)**
