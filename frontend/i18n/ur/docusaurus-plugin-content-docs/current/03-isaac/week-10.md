# ÛÙØªÛ 10: Sim-to-Real Transfer & Chapter 3 Project

## Ø¬Ø§Ø¦Ø²Û

Ø¨Ø§Ø¨ 3 Ú©Ø§ ÛŒÛ Ø¢Ø®Ø±ÛŒ ÛÙØªÛ sim-to-real transfer Ú©Û’ Ù†Ø§Ø²Ú© Ú†ÛŒÙ„Ù†Ø¬ Ø³Û’ Ù†Ù…Ù¹ØªØ§ ÛÛ’: simulation Ù…ÛŒÚº trained policies Ø§ÙˆØ± models Ú©Ùˆ Ø­Ù‚ÛŒÙ‚ÛŒ robots Ù¾Ø± Ú©Ø§Ù… Ú©Ø±Ù†Ø§Û” Ø¢Ù¾ Ø«Ø§Ø¨Øª Ø´Ø¯Û ØªÚ©Ù†ÛŒÚ©ÛŒÚº Ø³ÛŒÚ©Ú¾ÛŒÚº Ú¯Û’ØŒ transfer Ø­Ú©Ù…Øª Ø¹Ù…Ù„ÛŒ Ù†Ø§ÙØ° Ú©Ø±ÛŒÚº Ú¯Û’ØŒ Ø§ÙˆØ± Ø§ÛŒÚ© Ø¬Ø§Ù…Ø¹ Isaac Sim Ù¾Ø±ÙˆØ¬ÛŒÚ©Ù¹ Ù…Ú©Ù…Ù„ Ú©Ø±ÛŒÚº Ú¯Û’Û”

## Ø³ÛŒÚ©Ú¾Ù†Û’ Ú©Û’ Ù…Ù‚Ø§ØµØ¯

Ø§Ø³ ÛÙØªÛ’ Ú©Û’ Ø§Ø®ØªØªØ§Ù… ØªÚ©ØŒ Ø¢Ù¾ ÛŒÛ Ú©Ø± Ø³Ú©ÛŒÚº Ú¯Û’:

- Sim-to-real gap Ú©ÛŒ ÙˆØ¬ÙˆÛØ§Øª Ø³Ù…Ø¬Ú¾ÛŒÚº
- Ù…Ù†Ø¸Ù… Ø·Ø±ÛŒÙ‚Û’ Ø³Û’ domain randomization ØªÚ©Ù†ÛŒÚ©ÛŒÚº Ù„Ú¯Ø§Ø¦ÛŒÚº
- Physics calibration Ú©Û’ Ù„ÛŒÛ’ system identification Ù†Ø§ÙØ° Ú©Ø±ÛŒÚº
- Sim-to-real transfer Ú©Û’ Ø¨ÛØªØ±ÛŒÙ† Ø·Ø±ÛŒÙ‚Û’ Ø§Ø³ØªØ¹Ù…Ø§Ù„ Ú©Ø±ÛŒÚº
- Transfer Ú©ÛŒ Ú©Ø§Ù…ÛŒØ§Ø¨ÛŒ Ú©Ø§ Ù…Ù‚Ø¯Ø§Ø±ÛŒ Ø¬Ø§Ø¦Ø²Û Ù„ÛŒÚº
- Ø¨Ø§Ø¨ 3 Ú©Ø§ assessment Ù¾Ø±ÙˆØ¬ÛŒÚ©Ù¹ Ù…Ú©Ù…Ù„ Ú©Ø±ÛŒÚº
- Simulated policies Ú©Ùˆ Ø­Ù‚ÛŒÙ‚ÛŒ hardware Ù¾Ø± deploy Ú©Ø±ÛŒÚº (ØªØµÙˆØ±Ø§ØªÛŒ Ø·ÙˆØ± Ù¾Ø±)

## Sim-to-Real Gap

### Sim-to-Real Gap Ú©ÛŒØ§ ÛÛ’ØŸ

**ØªØ¹Ø±ÛŒÙ**: Simulation Ø³Û’ Ø­Ù‚ÛŒÙ‚ÛŒ Ø¯Ù†ÛŒØ§ Ù…ÛŒÚº policies/models Ù…Ù†ØªÙ‚Ù„ Ú©Ø±ØªÛ’ ÙˆÙ‚Øª Ú©Ø§Ø±Ú©Ø±Ø¯Ú¯ÛŒ Ù…ÛŒÚº Ú©Ù…ÛŒÛ”

**Ù…Ø«Ø§Ù„:**
- **Simulation Ù…ÛŒÚº**: Robot 95% ÙˆÙ‚Øª Ø§Ø´ÛŒØ§Ø¡ Ú©Ùˆ grasp Ú©Ø±ØªØ§ ÛÛ’
- **Ø­Ù‚ÛŒÙ‚ÛŒ robot Ù¾Ø±**: Robot 40% ÙˆÙ‚Øª Ø§Ø´ÛŒØ§Ø¡ Ú©Ùˆ grasp Ú©Ø±ØªØ§ ÛÛ’
- **Gap**: 55% Ú©Ø§Ø±Ú©Ø±Ø¯Ú¯ÛŒ Ú©Ø§ Ù†Ù‚ØµØ§Ù†

### Ø¨Ù†ÛŒØ§Ø¯ÛŒ ÙˆØ¬ÙˆÛØ§Øª

#### 1. Physics ØªØ¶Ø§Ø¯Ø§Øª

| Ø®Ø§ØµÛŒØª | Simulation | Ø­Ù‚ÛŒÙ‚Øª |
|----------|------------|---------|
| **Friction** | Ù…Ø³ØªÙ‚Ù„ØŒ Ø¢Ø³Ø§Ù† Ú©ÛŒØ§ ÛÙˆØ§ | Ù…ØªØºÛŒØ±ØŒ Ù¾ÛŒÚ†ÛŒØ¯Û |
| **Contact** | Penetration Ù¾Ø± Ù…Ø¨Ù†ÛŒ | DeformationØŒ slip |
| **Dynamics** | Deterministic | Stochastic |
| **Delays** | Ú©ÙˆØ¦ÛŒ Ù†ÛÛŒÚº | Motor lagØŒ sensor latency |
| **Noise** | Gaussian (Ø§Ú¯Ø± Ø´Ø§Ù…Ù„ ÛÙˆ) | Non-GaussianØŒ ÙˆÙ‚Øª Ø³Û’ Ù…Ù†Ø³Ù„Ú© |

#### 2. Ø¨ØµØ±ÛŒ ØªØ¶Ø§Ø¯Ø§Øª

- **Rendering**: Ú©Ø§Ù…Ù„ raytracing Ø¨Ù…Ù‚Ø§Ø¨Ù„Û Ø­Ù‚ÛŒÙ‚ÛŒ camera noise/blur
- **Lighting**: Ú©Ù†Ù¹Ø±ÙˆÙ„ Ø´Ø¯Û Ø¨Ù…Ù‚Ø§Ø¨Ù„Û Ù…ØªØºÛŒØ± Ù…Ø­ÛŒØ·ÛŒ Ø±ÙˆØ´Ù†ÛŒ
- **Textures**: ØµØ§Ù 3D models Ø¨Ù…Ù‚Ø§Ø¨Ù„Û Ú¯Ú¾Ø³Û’ ÛÙˆØ¦Û’/Ú¯Ù†Ø¯Û’ Ø­Ù‚ÛŒÙ‚ÛŒ Ø§Ø´ÛŒØ§Ø¡
- **Occlusion**: Ú©Ø§Ù…Ù„ Ø¨Ù…Ù‚Ø§Ø¨Ù„Û Ø¬Ø²ÙˆÛŒ sensor coverage

#### 3. Modeling Ú©ÛŒ ØºÙ„Ø·ÛŒØ§Úº

- **Ø¢Ø³Ø§Ù† Ú©ÛŒ ÛÙˆØ¦ÛŒ geometry**: CAD models Ø¨Ù…Ù‚Ø§Ø¨Ù„Û manufactured tolerances
- **Mass/inertia errors**: ØªØ®Ù…ÛŒÙ†Û Ø´Ø¯Û Ø¨Ù…Ù‚Ø§Ø¨Ù„Û Ø­Ù‚ÛŒÙ‚ÛŒ Ø®ØµÙˆØµÛŒØ§Øª
- **Sensor models**: Ù…Ø«Ø§Ù„ÛŒ Ø¨Ù…Ù‚Ø§Ø¨Ù„Û Ø­Ù‚ÛŒÙ‚ÛŒ sensor Ø®ØµÙˆØµÛŒØ§Øª
- **Actuation**: Ú©Ø§Ù…Ù„ motors Ø¨Ù…Ù‚Ø§Ø¨Ù„Û backlash/compliance

## Gap Ú©Ùˆ Ù¾Ø§Ù¹Ù†Ø§: Ø«Ø§Ø¨Øª Ø´Ø¯Û ØªÚ©Ù†ÛŒÚ©ÛŒÚº

### 1. Domain Randomization (DR)

**Ø®ÛŒØ§Ù„**: Simulation parameters Ú©Ùˆ randomize Ú©Ø±ÛŒÚº ØªØ§Ú©Û Ø­Ù‚ÛŒÙ‚ÛŒ Ø¯Ù†ÛŒØ§ ØµØ±Ù Ø§ÛŒÚ© Ø§ÙˆØ± variation ÛÙˆÛ”

**Randomize Ú©Ø±Ù†Û’ Ú©Û’ Ù„ÛŒÛ’ parameters:**

```python
import numpy as np

class DomainRandomizer:
    """Ø¬Ø§Ù…Ø¹ domain randomizationÛ”"""

    def __init__(self):
        self.params = {}

    def randomize_physics(self):
        """Physics Ø®ØµÙˆØµÛŒØ§Øª Ú©Ùˆ randomize Ú©Ø±ÛŒÚºÛ”"""
        # Friction coefficients
        self.params['friction'] = np.random.uniform(0.3, 1.5)

        # Mass (nominal Ú©Ø§ Â±20%)
        self.params['mass_scale'] = np.random.uniform(0.8, 1.2)

        # Joint damping
        self.params['joint_damping'] = np.random.uniform(0.01, 0.5)

        # Motor Ø·Ø§Ù‚Øª (Â±10%)
        self.params['motor_scale'] = np.random.uniform(0.9, 1.1)

        # Action delay (0-50ms)
        self.params['action_delay'] = np.random.uniform(0, 0.05)

    def randomize_vision(self):
        """Ø¨ØµØ±ÛŒ Ø¸ÛÙˆØ± Ú©Ùˆ randomize Ú©Ø±ÛŒÚºÛ”"""
        # Lighting
        self.params['light_intensity'] = np.random.uniform(1000, 50000)
        self.params['light_color'] = np.random.random(3)  # RGB

        # Camera
        self.params['exposure'] = np.random.uniform(0.5, 2.0)
        self.params['gamma'] = np.random.uniform(0.8, 1.2)
        self.params['noise_std'] = np.random.uniform(0, 0.02)  # Gaussian noise

        # Object Ú©ÛŒ Ø´Ú©Ù„
        self.params['object_color'] = np.random.random(3)
        self.params['object_texture'] = np.random.choice([
            "smooth", "rough", "metallic", "matte"
        ])

    def randomize_geometry(self):
        """Ø³Ø§Ø¦Ø² Ø§ÙˆØ± positions Ú©Ùˆ randomize Ú©Ø±ÛŒÚºÛ”"""
        # Object size (Â±5%)
        self.params['size_scale'] = np.random.uniform(0.95, 1.05)

        # Spawn position noise (Â±2cm)
        self.params['position_noise'] = np.random.uniform(-0.02, 0.02, size=3)

        # Orientation noise (Â±5 Ø¯Ø±Ø¬Û’)
        self.params['rotation_noise'] = np.random.uniform(-0.087, 0.087, size=3)

    def randomize_all(self):
        """ØªÙ…Ø§Ù… randomizations Ù„Ú¯Ø§Ø¦ÛŒÚºÛ”"""
        self.randomize_physics()
        self.randomize_vision()
        self.randomize_geometry()
        return self.params
```

**DR Ú©Û’ Ø³Ø§ØªÚ¾ Training:**

```python
def train_with_dr(env, num_episodes=10000):
    """Domain randomization Ú©Û’ Ø³Ø§ØªÚ¾ policy Ù¹Ø±ÛŒÙ†Ù†Ú¯ Ú©Ø±ÛŒÚºÛ”"""
    randomizer = DomainRandomizer()

    for episode in range(num_episodes):
        # Ø§Ø³ episode Ú©Û’ Ù„ÛŒÛ’ domain Ú©Ùˆ randomize Ú©Ø±ÛŒÚº
        params = randomizer.randomize_all()
        env.apply_randomization(params)

        # Episode Ú†Ù„Ø§Ø¦ÛŒÚº
        obs = env.reset()
        done = False

        while not done:
            action = policy(obs)
            obs, reward, done, info = env.step(action)

        if episode % 100 == 0:
            print(f"Episode {episode}: Avg reward = {avg_reward}")
```

### 2. System Identification

**ÛØ¯Ù**: Ø­Ù‚ÛŒÙ‚ÛŒ robot Ø®ØµÙˆØµÛŒØ§Øª Ú©ÛŒ Ù¾ÛŒÙ…Ø§Ø¦Ø´ Ú©Ø±ÛŒÚº Ø§ÙˆØ± simulation Ú©Ùˆ match Ú©Ø±ÛŒÚºÛ”

**Ù…Ø±Ø­Ù„Û 1: Friction Ú©ÛŒ Ø´Ù†Ø§Ø®Øª**

```python
def identify_friction(robot):
    """
    Ù…Ø³ØªÙ‚Ù„ force Ù„Ú¯Ø§Ø¦ÛŒÚºØŒ terminal velocity Ù†Ø§Ù¾ÛŒÚºÛ”
    friction_coef = force / (mass * g)
    """
    forces = [0.5, 1.0, 1.5, 2.0]  # Newtons
    velocities = []

    for force in forces:
        robot.apply_force(force)
        time.sleep(2.0)  # Terminal velocity ØªÚ© Ù¾ÛÙ†Ú†Ù†Û’ Ú©Ø§ Ø§Ù†ØªØ¸Ø§Ø± Ú©Ø±ÛŒÚº
        vel = robot.get_velocity()
        velocities.append(vel)

    # Linear regression: F = Î¼ * m * g + friction_loss
    # Ø¢Ø³Ø§Ù† Ú©ÛŒØ§ ÛÙˆØ§: Î¼ â‰ˆ F / (m * g)
    mass = 5.0  # kg (Ù…Ø¹Ù„ÙˆÙ…)
    g = 9.81
    friction_coef = np.mean(forces) / (mass * g)

    return friction_coef
```

**Ù…Ø±Ø­Ù„Û 2: Simulation Ú©Ùˆ Ø§Ù¾ ÚˆÛŒÙ¹ Ú©Ø±ÛŒÚº**

```xml
<!-- Ø´Ù†Ø§Ø®Øª Ø´Ø¯Û parameters Ú©Û’ Ø³Ø§ØªÚ¾ URDF/USD Ú©Ùˆ Ø§Ù¾ ÚˆÛŒÙ¹ Ú©Ø±ÛŒÚº -->
<gazebo reference="link">
  <mu1>0.68</mu1>  <!-- Ø´Ù†Ø§Ø®Øª Ø´Ø¯Û friction -->
  <mu2>0.68</mu2>
</gazebo>
```

### 3. Privileged Learning + Adaptation

**Ø®ÛŒØ§Ù„**: Ú©Ø§Ù…Ù„ sim Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ú©Û’ Ø³Ø§ØªÚ¾ Ù¹Ø±ÛŒÙ†Ù†Ú¯ Ú©Ø±ÛŒÚºØŒ Ù¾Ú¾Ø± deployment Ù¾Ø± adapt Ú©Ø±ÛŒÚºÛ”

```python
class PrivilegedPolicy:
    """Policy Ø¬Ùˆ training Ú©Û’ Ø¯ÙˆØ±Ø§Ù† privileged Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ø§Ø³ØªØ¹Ù…Ø§Ù„ Ú©Ø±ØªÛŒ ÛÛ’Û”"""

    def __init__(self):
        # Student policy (deployed)
        self.student = StudentPolicy(obs_dim=10, action_dim=4)

        # Teacher policy (ØµØ±Ù trainingØŒ privileged info Ø±Ú©Ú¾ØªÛŒ ÛÛ’)
        self.teacher = TeacherPolicy(obs_dim=10, priv_dim=20, action_dim=4)

    def train_step(self, obs, privileged_info, true_action):
        """Ø¯ÙˆÙ†ÙˆÚº policies Ù¹Ø±ÛŒÙ†Ù†Ú¯ Ú©Ø±ÛŒÚºÛ”"""
        # Teacher privileged info Ø§Ø³ØªØ¹Ù…Ø§Ù„ Ú©Ø±ØªØ§ ÛÛ’ (Ø­Ù‚ÛŒÙ‚ÛŒ object massØŒ frictionØŒ ÙˆØºÛŒØ±Û)
        teacher_action = self.teacher(obs, privileged_info)

        # Student privileged info Ú©Û’ Ø¨ØºÛŒØ± teacher Ø³Û’ match Ú©Ø±Ù†Û’ Ú©ÛŒ Ú©ÙˆØ´Ø´ Ú©Ø±ØªØ§ ÛÛ’
        student_action = self.student(obs)

        # Losses
        teacher_loss = mse_loss(teacher_action, true_action)
        distillation_loss = mse_loss(student_action, teacher_action)

        total_loss = teacher_loss + distillation_loss
        return total_loss

    def deploy(self, obs):
        """Deployment Ù¾Ø±ØŒ ØµØ±Ù student Ø§Ø³ØªØ¹Ù…Ø§Ù„ Ú©Ø±ÛŒÚºÛ”"""
        return self.student(obs)
```

**Privileged Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ú©ÛŒ Ù…Ø«Ø§Ù„ÛŒÚº:**
- Ø­Ù‚ÛŒÙ‚ÛŒ object massØŒ friction coefficients
- Ground-truth object poses (Ø¨Ù…Ù‚Ø§Ø¨Ù„Û noisy perception)
- Ù…Ø³ØªÙ‚Ø¨Ù„ Ú©Ø§ trajectory (Ù¾ÛŒØ´Ù† Ú¯ÙˆØ¦ÛŒ Ú©Û’ Ú©Ø§Ù…ÙˆÚº Ú©Û’ Ù„ÛŒÛ’)
- Ù¾ÙˆØ´ÛŒØ¯Û state (joint forcesØŒ contact points)

### 4. Ø­Ù‚ÛŒÙ‚ÛŒ Data Ù¾Ø± Fine-Tuning

**Ø­Ú©Ù…Øª Ø¹Ù…Ù„ÛŒ**: Sim Ù…ÛŒÚº Ù¹Ø±ÛŒÙ†Ù†Ú¯ Ú©Ø±ÛŒÚºØŒ Ú†Ú¾ÙˆÙ¹Û’ Ø­Ù‚ÛŒÙ‚ÛŒ dataset Ú©Û’ Ø³Ø§ØªÚ¾ fine-tune Ú©Ø±ÛŒÚºÛ”

```python
# Ù…Ø±Ø­Ù„Û 1: Simulation Ù…ÛŒÚº Pre-train Ú©Ø±ÛŒÚº (Ù„Ø§Ú©Ú¾ÙˆÚº samples)
policy = train_in_simulation(num_steps=10_000_000)

# Ù…Ø±Ø­Ù„Û 2: Ø­Ù‚ÛŒÙ‚ÛŒ data Ø¬Ù…Ø¹ Ú©Ø±ÛŒÚº (Ø³ÛŒÚ©Ú‘ÙˆÚº samples)
real_data = collect_real_robot_data(num_episodes=100)

# Ù…Ø±Ø­Ù„Û 3: Ø­Ù‚ÛŒÙ‚ÛŒ data Ù¾Ø± Fine-tune Ú©Ø±ÛŒÚº
policy = fine_tune(policy, real_data, num_epochs=50, lr=1e-5)
```

**Ø¨ÛØªØ±ÛŒÙ† Ø·Ø±ÛŒÙ‚Û’:**
- Catastrophic forgetting Ø³Û’ Ø¨Ú†Ù†Û’ Ú©Û’ Ù„ÛŒÛ’ Ú©Ù… learning rate Ø§Ø³ØªØ¹Ù…Ø§Ù„ Ú©Ø±ÛŒÚº
- Simulation Ø³Û’ 90% training data Ø±Ú©Ú¾ÛŒÚº
- Ø­Ù‚ÛŒÙ‚ÛŒ data Ú©Ùˆ failure modes Ù¾Ø± focus Ú©Ø±ÛŒÚº

### 5. Residual Learning

**Ø®ÛŒØ§Ù„**: Sim policy Ú©Û’ Ø§ÙˆÙ¾Ø± correction Ø³ÛŒÚ©Ú¾ÛŒÚºÛ”

```python
class ResidualPolicy:
    """Sim policy + Ø³ÛŒÚ©Ú¾Ø§ ÛÙˆØ§ residualÛ”"""

    def __init__(self, sim_policy):
        self.sim_policy = sim_policy  # Frozen
        self.residual_network = ResidualNet(obs_dim=10, action_dim=4)

    def forward(self, obs):
        # Sim policy action Ø­Ø§ØµÙ„ Ú©Ø±ÛŒÚº
        sim_action = self.sim_policy(obs)

        # Residual (correction) Ø­Ø³Ø§Ø¨ Ú©Ø±ÛŒÚº
        residual = self.residual_network(obs)

        # Ø­ØªÙ…ÛŒ action = sim + residual
        action = sim_action + residual
        return action
```

**Training:**
- Ø­Ù‚ÛŒÙ‚ÛŒ robot Ù¾Ø± residual network Ù¹Ø±ÛŒÙ†Ù†Ú¯ Ú©Ø±ÛŒÚº
- Sim policy Ú©Ø§ Ø¹Ù„Ù… Ø±Ú©Ú¾ØªØ§ ÛÛ’ØŒ ØµØ±Ù corrections Ø³ÛŒÚ©Ú¾ØªØ§ ÛÛ’

## Ù…Ù‚Ø¯Ø§Ø±ÛŒ Transfer Evaluation

### Ù…ÛŒÙ¹Ø±Ú©Ø³

**1. Success Rate**
```
Success Rate = (Ú©Ø§Ù…ÛŒØ§Ø¨ Ø¢Ø²Ù…Ø§Ø¦Ø´ÛŒÚº / Ú©Ù„ Ø¢Ø²Ù…Ø§Ø¦Ø´ÛŒÚº) Ã— 100%
```

**2. Sim-to-Real Performance Ratio**
```
Transfer Ratio = Ø­Ù‚ÛŒÙ‚ÛŒ Ú©Ø§Ø±Ú©Ø±Ø¯Ú¯ÛŒ / Sim Ú©Ø§Ø±Ú©Ø±Ø¯Ú¯ÛŒ
```
- Ratio = 1.0: Ú©Ø§Ù…Ù„ transfer
- Ratio < 0.7: Ø®Ø±Ø§Ø¨ transfer (Ø¨ÛØªØ±ÛŒ Ú©ÛŒ Ø¶Ø±ÙˆØ±Øª)
- Ratio > 0.9: Ø¨ÛØªØ±ÛŒÙ† transfer

**3. Sample Efficiency**
```
Samples Needed = 90% sim Ú©Ø§Ø±Ú©Ø±Ø¯Ú¯ÛŒ ØªÚ© Ù¾ÛÙ†Ú†Ù†Û’ Ú©Û’ Ù„ÛŒÛ’ Ø­Ù‚ÛŒÙ‚ÛŒ samples
```

**4. Task-Specific Ù…ÛŒÙ¹Ø±Ú©Ø³**
- **Grasping**: Grasp success rateØŒ grasp stability
- **Navigation**: ÛØ¯Ù ØªÚ© Ù¾ÛÙ†Ú†Ù†Û’ Ú©ÛŒ Ú©Ø§Ù…ÛŒØ§Ø¨ÛŒØŒ collision rate
- **Manipulation**: Task Ù…Ú©Ù…Ù„ Ú©Ø±Ù†Û’ Ú©Ø§ ÙˆÙ‚ØªØŒ precision

### Evaluation Ù¾Ø±ÙˆÙ¹ÙˆÚ©ÙˆÙ„

```python
def evaluate_transfer(policy, real_env, num_trials=100):
    """Sim-to-real transfer Ú©Ø§ Ø¬Ø§Ø¦Ø²Û Ù„ÛŒÚºÛ”"""
    successes = 0
    completion_times = []
    failures = {"collision": 0, "timeout": 0, "grasp_fail": 0}

    for trial in range(num_trials):
        obs = real_env.reset()
        done = False
        steps = 0

        while not done and steps < max_steps:
            action = policy(obs)
            obs, reward, done, info = real_env.step(action)
            steps += 1

        # Ù†ØªÛŒØ¬Û Ø±ÛŒÚ©Ø§Ø±Úˆ Ú©Ø±ÛŒÚº
        if info["success"]:
            successes += 1
            completion_times.append(steps)
        else:
            failure_type = info["failure_reason"]
            failures[failure_type] += 1

    # Ù…ÛŒÙ¹Ø±Ú©Ø³ Ø­Ø³Ø§Ø¨ Ú©Ø±ÛŒÚº
    success_rate = successes / num_trials
    avg_time = np.mean(completion_times) if completion_times else None

    return {
        "success_rate": success_rate,
        "avg_completion_time": avg_time,
        "failure_breakdown": failures
    }
```

## Ú©ÛŒØ³ Ø§Ø³Ù¹ÚˆÛŒ: Grasping Transfer

### Simulation Setup

```python
# Isaac Sim Ù…ÛŒÚº grasping policy Ù¹Ø±ÛŒÙ†Ù†Ú¯ Ú©Ø±ÛŒÚº
env = GraspingEnv(
    num_envs=2048,
    domain_randomization=True,
    object_types=["cube", "cylinder", "sphere", "irregular"],
    object_textures=textures_library,  # 100+ textures
    lighting_range=(5000, 50000),
    friction_range=(0.3, 1.5)
)

# PPO Ú©Û’ Ø³Ø§ØªÚ¾ Ù¹Ø±ÛŒÙ†Ù†Ú¯ Ú©Ø±ÛŒÚº
model = PPO("MultiInputPolicy", env, learning_rate=3e-4)
model.learn(total_timesteps=5_000_000)
```

### Ø­Ù‚ÛŒÙ‚ÛŒ Robot Deployment

```python
# Sim-trained policy Ù„ÙˆÚˆ Ú©Ø±ÛŒÚº
policy = load_policy("grasp_policy_sim.pth")

# Ø­Ù‚ÛŒÙ‚ÛŒ robot environment
real_env = RealRobotEnv(
    camera_topic="/camera/rgb",
    robot_ip="192.168.1.10"
)

# Evaluate Ú©Ø±ÛŒÚº
results = evaluate_transfer(policy, real_env, num_trials=50)
print(f"Sim-to-Real Success Rate: {results['success_rate']:.1%}")
```

**Ø¹Ø§Ù… Ù†ØªØ§Ø¦Ø¬:**
- Ú©ÙˆØ¦ÛŒ DR Ù†ÛÛŒÚº: 30-40% Ú©Ø§Ù…ÛŒØ§Ø¨ÛŒ
- DR Ú©Û’ Ø³Ø§ØªÚ¾: 70-85% Ú©Ø§Ù…ÛŒØ§Ø¨ÛŒ
- DR + Fine-tuning: 85-95% Ú©Ø§Ù…ÛŒØ§Ø¨ÛŒ

## Ø¨Ø§Ø¨ 3 Assessment Ù¾Ø±ÙˆØ¬ÛŒÚ©Ù¹

**Ú©Ø§Ù…**: manipulation task Ú©Û’ Ù„ÛŒÛ’ Ù…Ú©Ù…Ù„ Isaac Sim pipeline Ø¨Ù†Ø§Ø¦ÛŒÚº

### Ù¾Ø±ÙˆØ¬ÛŒÚ©Ù¹ Ú©ÛŒ Ø¶Ø±ÙˆØ±ÛŒØ§Øª (100 Ù¾ÙˆØ§Ø¦Ù†Ù¹Ø³)

#### Ø­ØµÛ 1: Scene Setup (15 Ù¾ÙˆØ§Ø¦Ù†Ù¹Ø³)
- Warehouse/factory environment Ø¨Ù†Ø§Ø¦ÛŒÚº
- Ø±Ú©Ø§ÙˆÙ¹ÛŒÚºØŒ Ù…Ø®ØªÙ„Ù lighting Ø´Ø§Ù…Ù„ Ú©Ø±ÛŒÚº
- Manipulation Ú©Û’ Ù„ÛŒÛ’ 3+ object types Ø´Ø§Ù…Ù„ Ú©Ø±ÛŒÚº
- Custom USD models (Ø¨ÙˆÙ†Ø³: CAD Ø³Û’ import Ú©Ø±ÛŒÚº)

#### Ø­ØµÛ 2: Synthetic Data Generation (25 Ù¾ÙˆØ§Ø¦Ù†Ù¹Ø³)
- Annotations Ú©Û’ Ø³Ø§ØªÚ¾ 5000+ ØªØµØ§ÙˆÛŒØ± ØªÛŒØ§Ø± Ú©Ø±ÛŒÚº
- Ø¬Ø§Ù…Ø¹ domain randomization Ù†Ø§ÙØ° Ú©Ø±ÛŒÚº:
  - Lighting (3+ Ø°Ø±Ø§Ø¦Ø¹ØŒ Ù…Ø®ØªÙ„Ù intensity/Ø±Ù†Ú¯)
  - Object posesØŒ Ø³Ø§Ø¦Ø²ØŒ textures
  - Camera parametersØŒ noise
- COCO ÛŒØ§ custom format Ù…ÛŒÚº export Ú©Ø±ÛŒÚº
- ØªÙ‚Ø³ÛŒÙ…: 80% trainØŒ 10% valØŒ 10% test

#### Ø­ØµÛ 3: Vision Model Training (25 Ù¾ÙˆØ§Ø¦Ù†Ù¹Ø³)
- Object detection ÛŒØ§ segmentation model Ù¹Ø±ÛŒÙ†Ù†Ú¯ Ú©Ø±ÛŒÚº
- ØµØ±Ù synthetic data Ø§Ø³ØªØ¹Ù…Ø§Ù„ Ú©Ø±ÛŒÚº
- Validation set Ù¾Ø± Ù…ÛŒÙ¹Ø±Ú©Ø³ Ø±Ù¾ÙˆØ±Ù¹ Ú©Ø±ÛŒÚº:
  - Detection Ú©Û’ Ù„ÛŒÛ’ mAP@0.5
  - Segmentation Ú©Û’ Ù„ÛŒÛ’ IoU
- Test images Ù¾Ø± predictions Ú©Ùˆ visualize Ú©Ø±ÛŒÚº

#### Ø­ØµÛ 4: RL Policy Training (25 Ù¾ÙˆØ§Ø¦Ù†Ù¹Ø³)
- Manipulation task Ú©ÛŒ ØªØ¹Ø±ÛŒÙ Ú©Ø±ÛŒÚº (pick-and-placeØŒ reachingØŒ ÙˆØºÛŒØ±Û)
- Domain randomization Ú©Û’ Ø³Ø§ØªÚ¾ environment Ù†Ø§ÙØ° Ú©Ø±ÛŒÚº
- RL algorithm Ú©Û’ Ø³Ø§ØªÚ¾ policy Ù¹Ø±ÛŒÙ†Ù†Ú¯ Ú©Ø±ÛŒÚº (PPO/SAC/ÙˆØºÛŒØ±Û)
- Sim Ù…ÛŒÚº Ú©Ø§Ù…ÛŒØ§Ø¨ task execution Ú©Ø§ Ù…Ø¸Ø§ÛØ±Û Ú©Ø±ÛŒÚº
- Training curvesØŒ success rate Ø±Ù¾ÙˆØ±Ù¹ Ú©Ø±ÛŒÚº

#### Ø­ØµÛ 5: Ø¯Ø³ØªØ§ÙˆÛŒØ²Ø§Øª (10 Ù¾ÙˆØ§Ø¦Ù†Ù¹Ø³)
- Setup instructions Ú©Û’ Ø³Ø§ØªÚ¾ README
- Architecture diagram (sceneØŒ sensorsØŒ algorithms)
- Training/evaluation Ø±Ù¾ÙˆØ±Ù¹ÛŒÚº
- Demo video (Ø²ÛŒØ§Ø¯Û Ø³Û’ Ø²ÛŒØ§Ø¯Û 5 Ù…Ù†Ù¹ Ù…Ú©Ù…Ù„ pipeline Ø¯Ú©Ú¾Ø§ØªÛ’ ÛÙˆØ¦Û’)

### ÚˆÛŒÙ„ÛŒÙˆØ± Ø§ÛŒØ¨Ù„Ø²

1. **GitHub Repository**:
   - ØªÙ…Ø§Ù… source code
   - Dataset generation scripts
   - Training scripts
   - Trained models/policies

2. **Dataset**:
   - ØªÛŒØ§Ø± Ø´Ø¯Û dataset Ú©Ø§ Ù„Ù†Ú© (cloud storage Ù¹Ú¾ÛŒÚ© ÛÛ’)
   - Repository Ù…ÛŒÚº 100 sample images

3. **Ø±Ù¾ÙˆØ±Ù¹** (PDFØŒ 3-5 ØµÙØ­Ø§Øª):
   - Ø·Ø±ÛŒÙ‚Û Ú©Ø§Ø±
   - Domain randomization Ø­Ú©Ù…Øª Ø¹Ù…Ù„ÛŒ
   - Training Ù…ÛŒÙ¹Ø±Ú©Ø³ Ø§ÙˆØ± curves
   - Ú†ÛŒÙ„Ù†Ø¬Ø² Ø§ÙˆØ± Ø­Ù„

4. **Demo Video**:
   - Scene walkthrough
   - Dataset generation process
   - Model/policy Ø¹Ù…Ù„ Ù…ÛŒÚº
   - Ù…Ù‚Ø¯Ø§Ø±ÛŒ Ù†ØªØ§Ø¦Ø¬

### Grading Rubric

| Ø¬Ø²Ùˆ | Ù¾ÙˆØ§Ø¦Ù†Ù¹Ø³ | Ù…Ø¹ÛŒØ§Ø± |
|-----------|--------|----------|
| **Scene Quality** | 15 | Ø­Ù‚ÛŒÙ‚Øª Ù¾Ø³Ù†Ø¯Ø§Ù†ÛØŒ Ù…ØªÙ†ÙˆØ¹ØŒ Ø§Ú†Ú¾ÛŒ Ø·Ø±Ø­ Ø±ÙˆØ´Ù† |
| **Dataset Quality** | 15 | Ø¨Ú‘Ø§ØŒ Ù…ØªÙ†ÙˆØ¹ØŒ ØµØ­ÛŒØ­ Ø·Ø±ÛŒÙ‚Û’ Ø³Û’ annotated |
| **DR Implementation** | 10 | Ø¬Ø§Ù…Ø¹ randomization |
| **Model Performance** | 15 | Ø¯Ø±Ø³ØªÚ¯ÛŒ Ú©ÛŒ Ø­Ø¯ÙˆØ¯ Ú©Ùˆ Ù¾ÙˆØ±Ø§ Ú©Ø±ØªØ§ ÛÛ’ |
| **RL Policy** | 15 | Sim Ù…ÛŒÚº Task Ú©Ø§Ù…ÛŒØ§Ø¨ÛŒ > 80% |
| **Code Quality** | 10 | ØµØ§ÙØŒ Ø¯Ø³ØªØ§ÙˆÛŒØ² Ø´Ø¯ÛØŒ Ù‚Ø§Ø¨Ù„ ØªÚ©Ø±Ø§Ø± |
| **Ø¯Ø³ØªØ§ÙˆÛŒØ²Ø§Øª** | 10 | ÙˆØ§Ø¶Ø­ØŒ Ù…Ú©Ù…Ù„ØŒ Ø§Ú†Ú¾ÛŒ Ø·Ø±Ø­ Ù„Ú©Ú¾Ø§ ÛÙˆØ§ |
| **ØªØ®Ù„ÛŒÙ‚ÛŒØª** | 10 | Ù†ÛŒØ§ Ø·Ø±ÛŒÙ‚Û ÛŒØ§ Ø§Ø¶Ø§ÙÛŒ Ø®ØµÙˆØµÛŒØ§Øª |

**Ø¨ÙˆÙ†Ø³ Ù…ÙˆØ§Ù‚Ø¹** (+Ø²ÛŒØ§Ø¯Û Ø³Û’ Ø²ÛŒØ§Ø¯Û 20 Ù¾ÙˆØ§Ø¦Ù†Ù¹Ø³):
- Ø­Ù‚ÛŒÙ‚ÛŒ robot Ù¾Ø± deploy Ú©Ø±ÛŒÚº (+15)
- Multi-task learning (+10)
- Custom physics simulation (+10)
- SLAM integration (+10)

### Ù…Ø«Ø§Ù„ Ú©Û’ Ù¾Ø±ÙˆØ¬ÛŒÚ©Ù¹Ø³

**Ù…Ø«Ø§Ù„ 1: Bin Picking**
- Task: Ø¨Ú©Ú¾Ø±Û’ ÛÙˆØ¦Û’ bin Ø³Û’ parts Ø§Ù¹Ú¾Ø§Ø¦ÛŒÚº
- Objects: Ù¾ÛŒÚ†ØŒ Ù†Ù¹ØŒ ÙˆØ§Ø´Ø± (Ù…Ø®ØªÙ„Ù Ø³Ø§Ø¦Ø²)
- Vision: Detection Ú©Û’ Ù„ÛŒÛ’ YOLOv8
- Policy: Pick-and-place Ú©Û’ Ù„ÛŒÛ’ PPO

**Ù…Ø«Ø§Ù„ 2: Quality Inspection**
- Task: ØªÛŒØ§Ø± Ø´Ø¯Û parts Ù¾Ø± Ø®Ø±Ø§Ø¨ÛŒÙˆÚº Ú©Ø§ Ù¾ØªÛ Ù„Ú¯Ø§Ø¦ÛŒÚº
- Dataset: Ø®Ø±Ø§Ø¨ÛŒ annotations Ú©Û’ Ø³Ø§ØªÚ¾ 10K ØªØµØ§ÙˆÛŒØ±
- Model: Semantic segmentation (U-Net)
- Deployment: Real-time inference Ú©Û’ Ù„ÛŒÛ’ ROS 2 node

**Ù…Ø«Ø§Ù„ 3: Warehouse Navigation**
- Task: Shelf ØªÚ© navigate Ú©Ø±ÛŒÚºØŒ Ú†ÛŒØ² Ø¨Ø§Ø²ÛŒØ§ÙØª Ú©Ø±ÛŒÚº
- Environment: Aisles Ú©Û’ Ø³Ø§ØªÚ¾ 50x50m warehouse
- Vision: Ø±Ú©Ø§ÙˆÙ¹ÙˆÚº Ø³Û’ Ø¨Ú†Ù†Û’ Ú©Û’ Ù„ÛŒÛ’ depth camera
- Policy: Navigation + grasping Ú©Û’ Ù„ÛŒÛ’ SAC

## ÛÙØªÛ 10 Ø¹Ù…Ù„ÛŒ Ù…Ø´Ù‚

**Ú©Ø§Ù…**: DR Ø­Ú©Ù…Øª Ø¹Ù…Ù„ÛŒÙˆÚº Ú©Ùˆ Ù†Ø§ÙØ° Ø§ÙˆØ± Ù…ÙˆØ§Ø²Ù†Û Ú©Ø±ÛŒÚº

1. **Ø¨ØºÛŒØ±** DR Ú©Û’ baseline policy Ù¹Ø±ÛŒÙ†Ù†Ú¯ Ú©Ø±ÛŒÚº
2. ØµØ±Ù lighting DR Ú©Û’ Ø³Ø§ØªÚ¾ policy Ù¹Ø±ÛŒÙ†Ù†Ú¯ Ú©Ø±ÛŒÚº
3. Ù…Ú©Ù…Ù„ DR Ú©Û’ Ø³Ø§ØªÚ¾ policy Ù¹Ø±ÛŒÙ†Ù†Ú¯ Ú©Ø±ÛŒÚº (lighting + physics + vision)
4. Ù…ØªØºÛŒØ± test environments Ù…ÛŒÚº ØªÛŒÙ†ÙˆÚº Ú©Ø§ Ø¬Ø§Ø¦Ø²Û Ù„ÛŒÚº
5. Ú©Ø§Ø±Ú©Ø±Ø¯Ú¯ÛŒ Ú©Ø§ Ù…ÙˆØ§Ø²Ù†Û Ø±Ù¾ÙˆØ±Ù¹ Ú©Ø±ÛŒÚº

**Ù…ØªÙˆÙ‚Ø¹ Ù†ØªÛŒØ¬Û**: Ù…Ú©Ù…Ù„ DR policy environment Ú©ÛŒ ØªØ¨Ø¯ÛŒÙ„ÛŒÙˆÚº Ú©Û’ Ù„ÛŒÛ’ Ø³Ø¨ Ø³Û’ Ø²ÛŒØ§Ø¯Û Ù…Ø¶Ø¨ÙˆØ· ÛÙˆÙ†ÛŒ Ú†Ø§ÛÛŒÛ’Û”

## ÙˆØ³Ø§Ø¦Ù„

- [Sim-to-Real Transfer Survey](https://arxiv.org/abs/2009.13303)
- [Domain Randomization for Transferring Deep Neural Networks](https://arxiv.org/abs/1703.06907)
- [Learning Dexterous In-Hand Manipulation](https://arxiv.org/abs/1808.00177) - OpenAI Ú©Ø§ Rubik's Cube
- [NVIDIA Isaac Sim Examples](https://github.com/NVIDIA-Omniverse/IsaacGymEnvs)
- [System Identification Techniques](https://stanford.edu/class/ee363/sysid.pdf)

## Ø¨Ø§Ø¨ 3 Ú©Ø§ Ø®Ù„Ø§ØµÛ

Ø¨Ø§Ø¨ 3 Ù…Ú©Ù…Ù„ Ú©Ø±Ù†Û’ Ù¾Ø± Ù…Ø¨Ø§Ø±Ú©Ø¨Ø§Ø¯! Ø¢Ù¾ Ù†Û’ Ø³ÛŒÚ©Ú¾Ø§ ÛÛ’:

âœ… NVIDIA Isaac Sim Ø³ÛŒÙ¹ Ø§Ù¾ Ø§ÙˆØ± navigation
âœ… USD scene creation Ø§ÙˆØ± robot import
âœ… Sensor data Ú©Û’ Ù„ÛŒÛ’ ROS 2 integration
âœ… Replicator Ú©Û’ Ø³Ø§ØªÚ¾ synthetic data generation
âœ… Domain randomization Ø­Ú©Ù…Øª Ø¹Ù…Ù„ÛŒ
âœ… Isaac Gym Ú©Û’ Ø³Ø§ØªÚ¾ Ø¨ÛØª Ø²ÛŒØ§Ø¯Û parallel RL
âœ… Sim-to-real transfer ØªÚ©Ù†ÛŒÚ©ÛŒÚº
âœ… Sim Ø³Û’ deployment ØªÚ© Ù…Ú©Ù…Ù„ ML pipeline

## Ø§Ú¯Ù„Û’ Ù‚Ø¯Ù…

**Ø¢Ú¯Û’ Ú©ÛŒØ§ ÛÛ’:**
- Ø¨Ø§Ø¨ 3 Ú©Ø§ assessment Ù¾Ø±ÙˆØ¬ÛŒÚ©Ù¹ Ù…Ú©Ù…Ù„ Ú©Ø±ÛŒÚº
- Ø¶Ø±ÙˆØ±Øª Ú©Û’ Ù…Ø·Ø§Ø¨Ù‚ Isaac Sim ØªØµÙˆØ±Ø§Øª Ú©Ø§ Ø¬Ø§Ø¦Ø²Û Ù„ÛŒÚº
- Ø¨Ø§Ø¨ 4 Ú©Û’ Ù„ÛŒÛ’ ØªÛŒØ§Ø±ÛŒ Ú©Ø±ÛŒÚº: [Vision-Language-Action Models & Capstone](../04-vla/index.md)

Ø¨Ø§Ø¨ 4 ÛØ± Ú†ÛŒØ² Ú©Ùˆ Ø§Ú©Ù¹Ú¾Ø§ Ú©Ø±Û’ Ú¯Ø§: multimodal AIØŒ end-to-end robot systemsØŒ Ø§ÙˆØ± Ø¢Ù¾ Ú©Ø§ Ø­ØªÙ…ÛŒ capstone Ù¾Ø±ÙˆØ¬ÛŒÚ©Ù¹!

---

## ğŸ“ ÛÙØªÛ ÙˆØ§Ø± Quiz

Ø§Ø³ ÛÙØªÛ’ Ú©Û’ Ù…ÙˆØ§Ø¯ Ú©ÛŒ Ø§Ù¾Ù†ÛŒ Ø³Ù…Ø¬Ú¾ Ú©Ùˆ Ø¬Ø§Ù†Ú†ÛŒÚº! Quiz multiple choice ÛÛ’ØŒ Ø®ÙˆØ¯Ú©Ø§Ø± Ø·ÙˆØ± Ù¾Ø± score ÛÙˆØªØ§ ÛÛ’ØŒ Ø§ÙˆØ± Ø¢Ù¾ Ú©Û’ Ù¾Ø§Ø³ 2 Ú©ÙˆØ´Ø´ÛŒÚº ÛÛŒÚºÛ”

**[ÛÙØªÛ 10 Quiz Ù„ÛŒÚº â†’](/quiz?week=10)**
