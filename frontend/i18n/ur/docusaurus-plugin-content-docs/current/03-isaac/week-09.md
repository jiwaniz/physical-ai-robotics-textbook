# ÛÙØªÛ 9: Synthetic Data Generation & Isaac Gym

## Ø¬Ø§Ø¦Ø²Û

ÛŒÛ ÛÙØªÛ Isaac Sim Ú©ÛŒ Ø¯Ùˆ Ø·Ø§Ù‚ØªÙˆØ± ØµÙ„Ø§Ø­ÛŒØªÙˆÚº Ú©Ùˆ Ø¯Ø±ÛŒØ§ÙØª Ú©Ø±ØªØ§ ÛÛ’: Vision models Ù¹Ø±ÛŒÙ†Ù†Ú¯ Ú©Ø±Ù†Û’ Ú©Û’ Ù„ÛŒÛ’ **Synthetic Data Generation** (SDG) Ø§ÙˆØ± Ø¨ÛØª Ø²ÛŒØ§Ø¯Û parallel reinforcement learning Ú©Û’ Ù„ÛŒÛ’ **Isaac Gym**Û” Ø¢Ù¾ Ø³ÛŒÚ©Ú¾ÛŒÚº Ú¯Û’ Ú©Û Ø­Ù‚ÛŒÙ‚Øª Ù¾Ø³Ù†Ø¯Ø§Ù†Û training datasets Ú©ÛŒØ³Û’ Ø¨Ù†Ø§Ø¦ÛŒÚº Ø§ÙˆØ± robot policies Ú©Ùˆ Ù…Ú©Ù…Ù„ Ø·ÙˆØ± Ù¾Ø± simulation Ù…ÛŒÚº Ú©ÛŒØ³Û’ train Ú©Ø±ÛŒÚºÛ”

## Ø³ÛŒÚ©Ú¾Ù†Û’ Ú©Û’ Ù…Ù‚Ø§ØµØ¯

Ø§Ø³ ÛÙØªÛ’ Ú©Û’ Ø§Ø®ØªØªØ§Ù… ØªÚ©ØŒ Ø¢Ù¾ ÛŒÛ Ú©Ø± Ø³Ú©ÛŒÚº Ú¯Û’:

- AI/ML Ú©Û’ Ù„ÛŒÛ’ synthetic data Ú©ÛŒ Ù‚Ø¯Ø± Ø³Ù…Ø¬Ú¾ÛŒÚº
- Dataset generation Ú©Û’ Ù„ÛŒÛ’ Isaac Sim Replicator Ø§Ø³ØªØ¹Ù…Ø§Ù„ Ú©Ø±ÛŒÚº
- Domain-randomized training data Ø¨Ù†Ø§Ø¦ÛŒÚº
- Annotated datasets ØªÛŒØ§Ø± Ú©Ø±ÛŒÚº (bounding boxesØŒ segmentation masks)
- Reinforcement learning Ú©Û’ Ù„ÛŒÛ’ Isaac Gym Ø³ÛŒÙ¹ Ø§Ù¾ Ú©Ø±ÛŒÚº
- Ø³Ø§Ø¯Û RL policy Ù¹Ø±ÛŒÙ†Ù†Ú¯ Ú©Ø±ÛŒÚº (reachingØŒ grasping)
- Sim-to-real transfer Ú©ÛŒ ØªÛŒØ§Ø±ÛŒ Ú©Ø§ Ø¬Ø§Ø¦Ø²Û Ù„ÛŒÚº

## Synthetic Data Ú©ÛŒÙˆÚºØŸ

### Robotics Ù…ÛŒÚº Data Ú©Ø§ Ù…Ø³Ø¦Ù„Û

**Ø±ÙˆØ§ÛŒØªÛŒ Ø·Ø±ÛŒÙ‚Û:**
1. Ø¬Ø³Ù…Ø§Ù†ÛŒ robot Ø¨Ù†Ø§Ø¦ÛŒÚº ($10K-$1M)
2. Ø¯Ø³ØªÛŒ Ø·ÙˆØ± Ù¾Ø± data Ø¬Ù…Ø¹ Ú©Ø±ÛŒÚº (ÛÙØªÛ’/Ù…ÛÛŒÙ†Û’)
3. Ø¯Ø³ØªÛŒ Ø·ÙˆØ± Ù¾Ø± data Ú©Ùˆ label Ú©Ø±ÛŒÚº (Ù…ÛÙ†Ú¯Ø§ØŒ ØºÙ„Ø·ÛŒ Ú©Ø§ Ø´Ú©Ø§Ø±)
4. Model Ù¹Ø±ÛŒÙ†Ù†Ú¯ Ú©Ø±ÛŒÚº
5. Ø¬Ø¨ model Ù†Ø¦Û’ Ù…Ù†Ø¸Ø±Ù†Ø§Ù…ÙˆÚº Ù…ÛŒÚº Ù†Ø§Ú©Ø§Ù… ÛÙˆ ØªÙˆ Ø¯ÛØ±Ø§Ø¦ÛŒÚº

**Synthetic data Ø·Ø±ÛŒÙ‚Û:**
1. Simulation Ø¨Ù†Ø§Ø¦ÛŒÚº (Ø¯Ù†)
2. Ø®ÙˆØ¯Ú©Ø§Ø± Ø·ÙˆØ± Ù¾Ø± Ù„Ø§Ú©Ú¾ÙˆÚº samples ØªÛŒØ§Ø± Ú©Ø±ÛŒÚº (Ú¯Ú¾Ù†Ù¹Û’)
3. Ø®ÙˆØ¯Ú©Ø§Ø± Ø·ÙˆØ± Ù¾Ø± Ú©Ø§Ù…Ù„ labels (Ù…ÙØª)
4. Model Ù¹Ø±ÛŒÙ†Ù†Ú¯ Ú©Ø±ÛŒÚº
5. Ù†Ø¦Û’ Ù…Ù†Ø¸Ø±Ù†Ø§Ù…ÙˆÚº Ú©Û’ Ù„ÛŒÛ’ domain randomization Ø´Ø§Ù…Ù„ Ú©Ø±ÛŒÚº

### Synthetic Data Ú©Û’ ÙÙˆØ§Ø¦Ø¯

| Ù¾ÛÙ„Ùˆ | Ø­Ù‚ÛŒÙ‚ÛŒ Data | Synthetic Data |
|--------|-----------|----------------|
| **Ù„Ø§Ú¯Øª** | $$$$ (hardwareØŒ Ù…Ø­Ù†Øª) | $ (ØµØ±Ù compute) |
| **Ø±ÙØªØ§Ø±** | Ø³Ø³Øª (Ø¬Ø³Ù…Ø§Ù†ÛŒ Ø¬Ù…Ø¹) | ØªÛŒØ² (parallel generation) |
| **Ù¾ÛŒÙ…Ø§Ù†Û** | ÛØ²Ø§Ø±ÙˆÚº ØªØµØ§ÙˆÛŒØ± | Ù„Ø§Ú©Ú¾ÙˆÚº ØªØµØ§ÙˆÛŒØ± |
| **Labels** | Ø¯Ø³ØªÛŒ ($0.10-$1/image) | Ø®ÙˆØ¯Ú©Ø§Ø± (Ù…ÙØªØŒ Ú©Ø§Ù…Ù„) |
| **ØªÙ†ÙˆØ¹** | Ø¬Ø³Ù…Ø§Ù†ÛŒ Ø³ÛŒÙ¹ Ø§Ù¾ Ø³Û’ Ù…Ø­Ø¯ÙˆØ¯ | Ù„Ø§Ù…Ø­Ø¯ÙˆØ¯ (domain randomization) |
| **Ø­ÙØ§Ø¸Øª** | Ù†Ù‚ØµØ§Ù† Ú©Ø§ Ø®Ø·Ø±Û | Ø®Ø·Ø±Û’ Ø³Û’ Ù¾Ø§Ú© |
| **Edge cases** | Ù¾Ú©Ú‘Ù†Ø§ Ù…Ø´Ú©Ù„ | Ø¨Ù†Ø§Ù†Ø§ Ø¢Ø³Ø§Ù† |

### Ú†ÛŒÙ„Ù†Ø¬Ø² Ø§ÙˆØ± Ø­Ù„

**Ú†ÛŒÙ„Ù†Ø¬ 1: Sim-to-Real Gap**
- Simulated ØªØµØ§ÙˆÛŒØ± Ø­Ù‚ÛŒÙ‚Øª Ú©Û’ Ù…Ù‚Ø§Ø¨Ù„Û’ Ù…ÛŒÚº "Ø¬Ø¹Ù„ÛŒ" Ù†Ø¸Ø± Ø¢ØªÛŒ ÛÛŒÚº
- **Ø­Ù„**: Domain randomization (lightingØŒ texturesØŒ camera params)

**Ú†ÛŒÙ„Ù†Ø¬ 2: Physics Mismatch**
- Simulated physics Ø­Ù‚ÛŒÙ‚ÛŒ Ø¯Ù†ÛŒØ§ Ø³Û’ Ù…Ø®ØªÙ„Ù ÛÛ’
- **Ø­Ù„**: System identificationØŒ Ø­Ù‚ÛŒÙ‚ÛŒ data Ù¾Ø± fine-tuning

**Ú†ÛŒÙ„Ù†Ø¬ 3: Simulation Ù¾Ø± Overfitting**
- Model sim Ù…ÛŒÚº Ú©Ø§Ù… Ú©Ø±ØªØ§ ÛÛ’ Ù„ÛŒÚ©Ù† Ø­Ù‚ÛŒÙ‚ÛŒ robot Ù¾Ø± Ù†Ø§Ú©Ø§Ù… ÛÙˆØªØ§ ÛÛ’
- **Ø­Ù„**: Ù…ØªÙ†ÙˆØ¹ randomizationØŒ sim-to-real transfer ØªÚ©Ù†ÛŒÚ©ÛŒÚº

## Isaac Sim Replicator

**Replicator** Isaac Sim Ú©Ø§ synthetic data generation framework ÛÛ’Û”

### Ø§ÛÙ… ØµÙ„Ø§Ø­ÛŒØªÛŒÚº

- **Randomization**: MaterialsØŒ lightingØŒ camera paramsØŒ object poses
- **Annotations**: Bounding boxesØŒ segmentationØŒ depthØŒ normals
- **Scalability**: Parallel Ù…ÛŒÚº ÛØ²Ø§Ø±ÙˆÚº ØªØµØ§ÙˆÛŒØ± ØªÛŒØ§Ø± Ú©Ø±ÛŒÚº
- **Formats**: COCOØŒ KITTIØŒ Custom JSON

### Replicator Workflow

```
1. Ø¨Ù†ÛŒØ§Ø¯ÛŒ scene Ø¨Ù†Ø§Ø¦ÛŒÚº
   â†“
2. Randomizers Ú©ÛŒ ØªØ¹Ø±ÛŒÙ Ú©Ø±ÛŒÚº (lightingØŒ materialsØŒ poses)
   â†“
3. Writers Ú©ÛŒ ØªØ¹Ø±ÛŒÙ Ú©Ø±ÛŒÚº (annotations Ù…Ø­ÙÙˆØ¸ Ú©Ø±ÛŒÚº)
   â†“
4. Generation loop Ú†Ù„Ø§Ø¦ÛŒÚº
   â†“
5. Dataset export Ú©Ø±ÛŒÚº
```

## Synthetic Dataset Ø¨Ù†Ø§Ù†Ø§

### Ù…Ø«Ø§Ù„: Object Detection Dataset

**ÛØ¯Ù**: Ù…ÛŒØ² Ù¾Ø± boxes Ú©Ø§ Ù¾ØªÛ Ù„Ú¯Ø§Ù†Û’ Ú©Û’ Ù„ÛŒÛ’ YOLOv8 Ù¹Ø±ÛŒÙ†Ù†Ú¯ Ú©Ø±ÛŒÚº

#### Ù…Ø±Ø­Ù„Û 1: Ø¨Ù†ÛŒØ§Ø¯ÛŒ Scene Ø¨Ù†Ø§Ø¦ÛŒÚº

```python
from omni.isaac.kit import SimulationApp
simulation_app = SimulationApp({"headless": True})  # Ø±ÙØªØ§Ø± Ú©Û’ Ù„ÛŒÛ’ Ú©ÙˆØ¦ÛŒ GUI Ù†ÛÛŒÚº

from omni.isaac.core import World
from omni.isaac.core.objects import DynamicCuboid, VisualCuboid
from omni.isaac.core.prims import GeometryPrim
from omni.replicator.core import randomizer, Writer
import omni.replicator.core as rep
import numpy as np

# Ø¯Ù†ÛŒØ§ Ø¨Ù†Ø§Ø¦ÛŒÚº
world = World(stage_units_in_meters=1.0)
world.scene.add_default_ground_plane()

# Ù…ÛŒØ² Ø¨Ù†Ø§Ø¦ÛŒÚº
table = world.scene.add(
    VisualCuboid(
        prim_path="/World/Table",
        name="table",
        position=np.array([0, 0, 0.5]),
        size=np.array([1.0, 1.0, 0.05]),
        color=np.array([0.5, 0.3, 0.1])  # Ø¨Ú¾ÙˆØ±Ø§
    )
)

# Ù…ÛŒØ² Ú©Ùˆ Ø¯ÛŒÚ©Ú¾Ù†Û’ ÙˆØ§Ù„Ø§ camera Ø¨Ù†Ø§Ø¦ÛŒÚº
camera = rep.create.camera(
    position=(0, -2, 1.5),
    look_at=(0, 0, 0.5)
)

# Ø±ÙˆØ´Ù†ÛŒ Ø¨Ù†Ø§Ø¦ÛŒÚº
light = rep.create.light(
    light_type="Sphere",
    intensity=30000,
    position=(2, 2, 3),
    scale=0.5
)
```

#### Ù…Ø±Ø­Ù„Û 2: Randomizers Ú©ÛŒ ØªØ¹Ø±ÛŒÙ Ú©Ø±ÛŒÚº

```python
import omni.replicator.core as rep

# Ø¢Ø¨Ø¬ÛŒÚ©Ù¹Ø³ Ú©ÛŒ positions Ú©Ùˆ randomize Ú©Ø±ÛŒÚº
def randomize_objects():
    """Ù…ÛŒØ² Ù¾Ø± 1-5 boxes Ø¨Û’ ØªØ±ØªÛŒØ¨ Ø·ÙˆØ± Ù¾Ø± Ø±Ú©Ú¾ÛŒÚºÛ”"""
    num_objects = np.random.randint(1, 6)

    for i in range(num_objects):
        # Ù…ÛŒØ² Ù¾Ø± Ø¨Û’ ØªØ±ØªÛŒØ¨ position
        x = np.random.uniform(-0.4, 0.4)
        y = np.random.uniform(-0.4, 0.4)
        z = 0.525  # Ù…ÛŒØ² Ø³Û’ Ø¨Ø§Ù„Ú©Ù„ Ø§ÙˆÙ¾Ø±

        # Ø¨Û’ ØªØ±ØªÛŒØ¨ Ø³Ø§Ø¦Ø²
        size = np.random.uniform(0.05, 0.15)

        # Ø¨Û’ ØªØ±ØªÛŒØ¨ Ø±Ù†Ú¯
        color = np.random.random(3)

        # Box Ø¨Ù†Ø§Ø¦ÛŒÚº
        box = world.scene.add(
            DynamicCuboid(
                prim_path=f"/World/Box_{i}",
                name=f"box_{i}",
                position=np.array([x, y, z]),
                size=np.array([size, size, size]),
                color=color
            )
        )

# Lighting Ú©Ùˆ randomize Ú©Ø±ÛŒÚº
def randomize_lighting():
    """Ø±ÙˆØ´Ù†ÛŒ Ú©ÛŒ Ø´Ø¯Øª Ø§ÙˆØ± position Ù…ÛŒÚº ØªØ¨Ø¯ÛŒÙ„ÛŒ Ú©Ø±ÛŒÚºÛ”"""
    intensity = np.random.uniform(20000, 40000)
    x = np.random.uniform(-3, 3)
    y = np.random.uniform(-3, 3)
    z = np.random.uniform(2, 4)

    # Ø±ÙˆØ´Ù†ÛŒ Ú©Ùˆ Ø§Ù¾ ÚˆÛŒÙ¹ Ú©Ø±ÛŒÚº (replicator API Ø§Ø³ØªØ¹Ù…Ø§Ù„ Ú©Ø±ÛŒÚº)
    with rep.new_layer():
        light = rep.get.prims(path_pattern="/World/Lights/*")
        with light:
            rep.modify.pose(position=(x, y, z))
            rep.modify.attribute("intensity", intensity)

# Camera Ú©Ùˆ randomize Ú©Ø±ÛŒÚº
def randomize_camera():
    """Ù…ÛŒØ² Ú©Û’ Ú¯Ø±Ø¯ camera position Ù…ÛŒÚº ØªØ¨Ø¯ÛŒÙ„ÛŒ Ú©Ø±ÛŒÚºÛ”"""
    # Ú©Ø±ÙˆÛŒ coordinates
    radius = np.random.uniform(1.5, 2.5)
    theta = np.random.uniform(-np.pi/4, np.pi/4)  # Â±45 Ø¯Ø±Ø¬Û’
    phi = np.random.uniform(np.pi/6, np.pi/3)     # 30-60 Ø¯Ø±Ø¬Û’ Ø¨Ù„Ù†Ø¯ÛŒ

    x = radius * np.cos(theta) * np.cos(phi)
    y = radius * np.sin(theta) * np.cos(phi)
    z = radius * np.sin(phi)

    with rep.new_layer():
        camera = rep.get.prims(path_pattern="/World/Camera")
        with camera:
            rep.modify.pose(position=(x, y, z), look_at=(0, 0, 0.5))
```

#### Ù…Ø±Ø­Ù„Û 3: Annotators Ø±Ø¬Ø³Ù¹Ø± Ú©Ø±ÛŒÚº

```python
# Annotations ÙØ¹Ø§Ù„ Ú©Ø±ÛŒÚº
rp = rep.create.render_product(camera, (640, 480))

# RGB ØªØµØ§ÙˆÛŒØ±
rgb_annot = rep.AnnotatorRegistry.get_annotator("rgb")
rgb_annot.attach(rp)

# Bounding boxes (2D)
bbox_2d_annot = rep.AnnotatorRegistry.get_annotator("bounding_box_2d_tight")
bbox_2d_annot.attach(rp)

# Semantic segmentation
semantic_annot = rep.AnnotatorRegistry.get_annotator("semantic_segmentation")
semantic_annot.attach(rp)

# Depth
depth_annot = rep.AnnotatorRegistry.get_annotator("distance_to_camera")
depth_annot.attach(rp)
```

#### Ù…Ø±Ø­Ù„Û 4: Custom Writer (COCO Format)

```python
import omni.replicator.core as rep
import json
import os
from PIL import Image

class COCOWriter(rep.Writer):
    """COCO format Ù…ÛŒÚº dataset export Ú©Ø±ÛŒÚºÛ”"""

    def __init__(self, output_dir):
        super().__init__()
        self.output_dir = output_dir
        os.makedirs(output_dir, exist_ok=True)
        os.makedirs(f"{output_dir}/images", exist_ok=True)

        self.coco_data = {
            "images": [],
            "annotations": [],
            "categories": [{"id": 1, "name": "box"}]
        }
        self.image_id = 0
        self.annot_id = 0

    def write(self, data):
        """ÛØ± frame Ú©Û’ Ù„ÛŒÛ’ Ú©Ø§Ù„ Ú©ÛŒØ§ Ø¬Ø§ØªØ§ ÛÛ’Û”"""
        # RGB image Ù…Ø­ÙÙˆØ¸ Ú©Ø±ÛŒÚº
        rgb = data["rgb"]
        img_filename = f"image_{self.image_id:06d}.png"
        img_path = f"{self.output_dir}/images/{img_filename}"
        Image.fromarray(rgb).save(img_path)

        # Image metadata Ø´Ø§Ù…Ù„ Ú©Ø±ÛŒÚº
        self.coco_data["images"].append({
            "id": self.image_id,
            "file_name": img_filename,
            "width": rgb.shape[1],
            "height": rgb.shape[0]
        })

        # Bounding box annotations Ø´Ø§Ù…Ù„ Ú©Ø±ÛŒÚº
        bboxes = data["bounding_box_2d_tight"]
        for bbox in bboxes:
            x_min, y_min, x_max, y_max = bbox
            width = x_max - x_min
            height = y_max - y_min

            self.coco_data["annotations"].append({
                "id": self.annot_id,
                "image_id": self.image_id,
                "category_id": 1,
                "bbox": [x_min, y_min, width, height],
                "area": width * height,
                "iscrowd": 0
            })
            self.annot_id += 1

        self.image_id += 1

    def on_final_frame(self):
        """Ø¢Ø®Ø±ÛŒ frame Ú©Û’ Ø¨Ø¹Ø¯ Ú©Ø§Ù„ Ú©ÛŒØ§ Ø¬Ø§ØªØ§ ÛÛ’Û”"""
        # COCO JSON Ù…Ø­ÙÙˆØ¸ Ú©Ø±ÛŒÚº
        with open(f"{self.output_dir}/annotations.json", "w") as f:
            json.dump(self.coco_data, f, indent=2)

        print(f"Dataset saved to {self.output_dir}")
        print(f"Total images: {self.image_id}")
        print(f"Total annotations: {self.annot_id}")

# Writer Ø±Ø¬Ø³Ù¹Ø± Ú©Ø±ÛŒÚº
writer = COCOWriter(output_dir="./dataset_boxes")
writer.attach(rp)
```

#### Ù…Ø±Ø­Ù„Û 5: Generation Ú†Ù„Ø§Ø¦ÛŒÚº

```python
# Generation loop
num_frames = 1000

world.reset()

for i in range(num_frames):
    # Scene Ú©Ùˆ randomize Ú©Ø±ÛŒÚº
    randomize_objects()
    randomize_lighting()
    randomize_camera()

    # Physics step (Ø§Ø´ÛŒØ§Ø¡ Ú©Ùˆ settle ÛÙˆÙ†Û’ Ø¯ÛŒÚº)
    for _ in range(10):
        world.step(render=False)

    # Frame capture Ú©Ø±ÛŒÚº
    world.step(render=True)

    # Replicator write Ú©Ùˆ trigger Ú©Ø±ÛŒÚº
    rep.orchestrator.step()

    if i % 100 == 0:
        print(f"Generated {i}/{num_frames} frames")

# Finalize
writer.on_final_frame()
simulation_app.close()
```

**Ù†ØªÛŒØ¬Û**: YOLO Ù¹Ø±ÛŒÙ†Ù†Ú¯ Ú©Û’ Ù„ÛŒÛ’ ØªÛŒØ§Ø± COCO annotations Ú©Û’ Ø³Ø§ØªÚ¾ 1000 ØªØµØ§ÙˆÛŒØ±!

## Domain Randomization Ú©Û’ Ø¨ÛØªØ±ÛŒÙ† Ø·Ø±ÛŒÙ‚Û’

### 1. Lighting Randomization

```python
# Ø¨Û’ ØªØ±ØªÛŒØ¨ Ø±Ù†Ú¯ÙˆÚº Ú©Û’ Ø³Ø§ØªÚ¾ Ù…ØªØ¹Ø¯Ø¯ Ø±ÙˆØ´Ù†ÛŒØ§Úº
for i in range(3):
    color = np.random.random(3)
    intensity = np.random.uniform(10000, 50000)
    position = np.random.uniform(-5, 5, size=3)

    light = rep.create.light(
        light_type="Sphere",
        intensity=intensity,
        color=color,
        position=position
    )
```

### 2. Texture Randomization

```python
# Ø¢Ø¨Ø¬ÛŒÚ©Ù¹Ø³ Ù¾Ø± Ø¨Û’ ØªØ±ØªÛŒØ¨ materials Ù„Ú¯Ø§Ø¦ÛŒÚº
materials = [
    "omni://localhost/NVIDIA/Materials/vMaterials_2/Ground/textures/aggregate_exposed_diff.jpg",
    "omni://localhost/NVIDIA/Materials/vMaterials_2/Wood/textures/wood_cherry_diff.jpg",
    # Ù…Ø²ÛŒØ¯ material paths Ø´Ø§Ù…Ù„ Ú©Ø±ÛŒÚº
]

def randomize_materials():
    boxes = rep.get.prims(semantics=[("class", "box")])
    with boxes:
        rep.randomizer.materials(materials)
```

### 3. Camera Randomization

```python
# Ø­Ù‚ÛŒÙ‚ÛŒ camera noise Ú©ÛŒ Ù†Ù‚Ù„ Ú©Ø±ÛŒÚº
with camera:
    # Motion blur
    rep.modify.attribute("motion_blur:enable", True)
    rep.modify.attribute("motion_blur:intensity", np.random.uniform(0, 0.5))

    # Exposure
    rep.modify.attribute("exposure", np.random.uniform(0.5, 2.0))

    # Focal length (FOV variation)
    rep.modify.attribute("focalLength", np.random.uniform(18, 55))
```

### 4. Background Randomization

```python
# Ø¨Û’ ØªØ±ØªÛŒØ¨ HDRI backgrounds Ø§Ø³ØªØ¹Ù…Ø§Ù„ Ú©Ø±ÛŒÚº
hdris = [
    "omniverse://localhost/NVIDIA/Assets/Skies/Indoor/ZetoCG_com_WarehouseInterior2.hdr",
    "omniverse://localhost/NVIDIA/Assets/Skies/Outdoor/kloppenheim_06_4k.hdr",
]

with rep.new_layer():
    dome_light = rep.create.light(light_type="Dome")
    with dome_light:
        rep.randomizer.texture(hdris)
```

## Isaac Gym: Ø¨ÛØª Ø²ÛŒØ§Ø¯Û Parallel RL

**Isaac Gym** Ø§ÛŒÚ© GPU Ù¾Ø± ÛØ²Ø§Ø±ÙˆÚº robot policies Ú©ÛŒ Ø¨ÛŒÚ© ÙˆÙ‚Øª Ù¹Ø±ÛŒÙ†Ù†Ú¯ Ú©Ùˆ Ù…Ù…Ú©Ù† Ø¨Ù†Ø§ØªØ§ ÛÛ’Û”

### Ø§ÛÙ… ØªØµÙˆØ±Ø§Øª

- **Vectorized environments**: Ø¨ÛŒÚ© ÙˆÙ‚Øª 1000+ instances Ú†Ù„Ø§Ø¦ÛŒÚº
- **GPU physics**: GPU Ù¾Ø± ØªÙ…Ø§Ù… simulation (Ú©ÙˆØ¦ÛŒ CPU bottleneck Ù†ÛÛŒÚº)
- **GPU tensors**: Observations/actions GPU Ù¾Ø± Ø±ÛØªÛ’ ÛÛŒÚº (Ú©ÙˆØ¦ÛŒ CPUâ†”GPU transfer Ù†ÛÛŒÚº)
- **Fast**: Ø¯Ù†ÙˆÚº Ú©Û’ Ø¨Ø¬Ø§Ø¦Û’ Ù…Ù†Ù¹ÙˆÚº Ù…ÛŒÚº policies Ù¹Ø±ÛŒÙ†Ù†Ú¯ Ú©Ø±ÛŒÚº

### Isaac Gym Ø¨Ù…Ù‚Ø§Ø¨Ù„Û Ø±ÙˆØ§ÛŒØªÛŒ RL

| Ù…ÛŒÙ¹Ø±Ú© | Ø±ÙˆØ§ÛŒØªÛŒ (CPU) | Isaac Gym (GPU) |
|--------|-------------------|-----------------|
| **Parallel envs** | 8-16 | 1024-8192 |
| **Timesteps/sec** | 1000-5000 | 100K-1M |
| **Training time (Reach)** | 24 Ú¯Ú¾Ù†Ù¹Û’ | 5 Ù…Ù†Ù¹ |
| **Hardware** | Multi-core CPU | Single RTX GPU |

### Ø³Ø§Ø¯Û Reach Task

**ÛØ¯Ù**: Ø¨Û’ ØªØ±ØªÛŒØ¨ ÛØ¯Ù positions ØªÚ© Ù¾ÛÙ†Ú†Ù†Û’ Ú©Û’ Ù„ÛŒÛ’ robot arm Ù¹Ø±ÛŒÙ†Ù†Ú¯ Ú©Ø±ÛŒÚº

#### Environment Setup

```python
from omni.isaac.gym import VecEnvBase
import torch
import numpy as np

class ReachEnv(VecEnvBase):
    """Ø³Ø§Ø¯Û reaching taskÛ”"""

    def __init__(self, num_envs=1024, device="cuda:0"):
        self.num_envs = num_envs
        self.device = device

        # Observation: [joint positions (7), target position (3)]
        self.num_obs = 10

        # Action: target joint positions (7)
        self.num_actions = 7

        super().__init__(num_envs=num_envs)

        # Targets Ø´Ø±ÙˆØ¹ Ú©Ø±ÛŒÚº
        self.targets = torch.zeros((num_envs, 3), device=device)

    def reset(self):
        """ØªÙ…Ø§Ù… environments Ú©Ùˆ reset Ú©Ø±ÛŒÚºÛ”"""
        # ÛØ¯Ù positions Ú©Ùˆ randomize Ú©Ø±ÛŒÚº
        self.targets = torch.rand((self.num_envs, 3), device=self.device)
        self.targets[:, 0] = self.targets[:, 0] * 0.6 + 0.2  # X: 0.2-0.8
        self.targets[:, 1] = (self.targets[:, 1] - 0.5) * 0.6 # Y: -0.3-0.3
        self.targets[:, 2] = self.targets[:, 2] * 0.5 + 0.2   # Z: 0.2-0.7

        # Robot Ú©Ùˆ home position Ù¾Ø± reset Ú©Ø±ÛŒÚº
        home_joints = torch.tensor([0, -1.0, 0, -2.2, 0, 2.4, 0.8], device=self.device)
        self.joint_positions = home_joints.repeat(self.num_envs, 1)

        return self.get_observations()

    def get_observations(self):
        """Ù…ÙˆØ¬ÙˆØ¯Û observations Ø­Ø§ØµÙ„ Ú©Ø±ÛŒÚºÛ”"""
        # Joint positions Ø§ÙˆØ± target Ú©Ùˆ concatenate Ú©Ø±ÛŒÚº
        obs = torch.cat([self.joint_positions, self.targets], dim=1)
        return obs

    def step(self, actions):
        """Actions apply Ú©Ø±ÛŒÚº Ø§ÙˆØ± simulation step Ú©Ø±ÛŒÚºÛ”"""
        # Actions target joint positions ÛÛŒÚº
        self.joint_positions = actions

        # End-effector position Ø­Ø§ØµÙ„ Ú©Ø±ÛŒÚº (Ø¢Ø³Ø§Ù† Ú©ÛŒØ§ ÛÙˆØ§ - Ø­Ù‚ÛŒÙ‚Øª Ù…ÛŒÚº forward kinematics Ø§Ø³ØªØ¹Ù…Ø§Ù„ Ú©Ø±ÛŒÚº)
        ee_pos = self.compute_ee_position(self.joint_positions)

        # Reward Ø­Ø³Ø§Ø¨ Ú©Ø±ÛŒÚº: ÛØ¯Ù Ø³Û’ Ù…Ù†ÙÛŒ ÙØ§ØµÙ„Û
        distance = torch.norm(ee_pos - self.targets, dim=1)
        rewards = -distance

        # Episode Ù…Ú©Ù…Ù„ Ø§Ú¯Ø± ÛØ¯Ù ØªÚ© Ù¾ÛÙ†Ú† Ú¯ÛŒØ§ (ÙØ§ØµÙ„Û < 0.05m)
        dones = distance < 0.05

        # Ù†Ø¦Û’ observations Ø­Ø§ØµÙ„ Ú©Ø±ÛŒÚº
        obs = self.get_observations()

        return obs, rewards, dones, {}

    def compute_ee_position(self, joint_positions):
        """Ø¢Ø³Ø§Ù† Ú©ÛŒØ§ ÛÙˆØ§ forward kinematicsÛ”"""
        # Ø­Ù‚ÛŒÙ‚Øª Ù…ÛŒÚºØŒ Ù…Ù†Ø§Ø³Ø¨ FK Ø§Ø³ØªØ¹Ù…Ø§Ù„ Ú©Ø±ÛŒÚº
        # ÛŒÛØ§ÚºØŒ ØµØ±Ù Ù…Ø¸Ø§ÛØ±Û’ Ú©Û’ Ù„ÛŒÛ’ ØªØ®Ù…ÛŒÙ†Û
        return torch.rand((self.num_envs, 3), device=self.device)
```

#### PPO Ú©Û’ Ø³Ø§ØªÚ¾ Training

```python
from stable_baselines3 import PPO
from stable_baselines3.common.vec_env import VecNormalize

# Environment Ø¨Ù†Ø§Ø¦ÛŒÚº
env = ReachEnv(num_envs=2048)

# Observations Ø§ÙˆØ± rewards Ú©Ùˆ normalize Ú©Ø±ÛŒÚº
env = VecNormalize(env, norm_obs=True, norm_reward=True)

# PPO agent Ø¨Ù†Ø§Ø¦ÛŒÚº
model = PPO(
    "MlpPolicy",
    env,
    learning_rate=3e-4,
    n_steps=16,  # ØªÛŒØ² updates Ú©Û’ Ù„ÛŒÛ’ Ú†Ú¾ÙˆÙ¹Ø§
    batch_size=4096,
    n_epochs=10,
    gamma=0.99,
    gae_lambda=0.95,
    clip_range=0.2,
    ent_coef=0.0,
    verbose=1,
    device="cuda"
)

# Train Ú©Ø±ÛŒÚº
model.learn(total_timesteps=1_000_000)

# Model Ù…Ø­ÙÙˆØ¸ Ú©Ø±ÛŒÚº
model.save("reach_policy")
```

**Training RTX 3080 Ù¾Ø± ~5 Ù…Ù†Ù¹ Ù…ÛŒÚº Ù…Ú©Ù…Ù„ ÛÙˆØªÛŒ ÛÛ’!**

### Ø­Ù‚ÛŒÙ‚ÛŒ Isaac Gym Ù…Ø«Ø§Ù„ (Cartpole)

Isaac Gym Ù…ÛŒÚº Ù¾ÛÙ„Û’ Ø³Û’ Ø¨Ù†Ø§Ø¦Û’ ÛÙˆØ¦Û’ tasks Ø´Ø§Ù…Ù„ ÛÛŒÚº:

```bash
cd ~/.local/share/ov/pkg/isaac_sim-2023.1.1/standalone_examples/api/omni.isaac.gym

# Cartpole Ù…Ø«Ø§Ù„ Ú†Ù„Ø§Ø¦ÛŒÚº
python cartpole.py
```

Parallel Ù…ÛŒÚº 2048 cartpoles Ù¹Ø±ÛŒÙ†Ù†Ú¯ Ú©Ø±ØªØ§ ÛÛ’!

## ÛÙØªÛ 9 Ø¹Ù…Ù„ÛŒ Ù¾Ø±ÙˆØ¬ÛŒÚ©Ù¹

**Ú©Ø§Ù…**: Synthetic dataset Ø¨Ù†Ø§Ø¦ÛŒÚº Ø§ÙˆØ± Ø³Ø§Ø¯Û model Ù¹Ø±ÛŒÙ†Ù†Ú¯ Ú©Ø±ÛŒÚº

**Ø­ØµÛ 1: Synthetic Dataset (50 Ù¾ÙˆØ§Ø¦Ù†Ù¹Ø³)**
- Scene: 3-6 Ø±Ù†Ú¯ÛŒÙ† cubes Ú©Û’ Ø³Ø§ØªÚ¾ Ù…ÛŒØ²
- Randomization: Lighting (3 Ø°Ø±Ø§Ø¦Ø¹)ØŒ cube positionsØŒ cube Ø±Ù†Ú¯
- 2000 ØªØµØ§ÙˆÛŒØ± ØªÛŒØ§Ø± Ú©Ø±ÛŒÚº (640x480)
- Annotations: COCO format Ù…ÛŒÚº bounding boxes
- Dataset Ú©Ùˆ disk Ù¾Ø± Ù…Ø­ÙÙˆØ¸ Ú©Ø±ÛŒÚº

**Ø­ØµÛ 2: Model Training (50 Ù¾ÙˆØ§Ø¦Ù†Ù¹Ø³)**
- Synthetic data Ù¾Ø± YOLOv8 ÛŒØ§ Faster R-CNN Ù¹Ø±ÛŒÙ†Ù†Ú¯ Ú©Ø±ÛŒÚº
- 200-image validation set Ù¾Ø± evaluate Ú©Ø±ÛŒÚº
- mAP (mean Average Precision) Ø±Ù¾ÙˆØ±Ù¹ Ú©Ø±ÛŒÚº
- Sim-to-real gap Ú©Ø§ Ø¬Ø§Ø¦Ø²Û Ù„ÛŒÙ†Û’ Ú©Û’ Ù„ÛŒÛ’ Ø­Ù‚ÛŒÙ‚ÛŒ ØªØµØ§ÙˆÛŒØ± Ù¾Ø± Ø¬Ø§Ù†Ú† Ú©Ø±ÛŒÚº (Ø§Ú¯Ø± Ø¯Ø³ØªÛŒØ§Ø¨ ÛÙˆÚº)

**ÚˆÛŒÙ„ÛŒÙˆØ± Ø§ÛŒØ¨Ù„Ø²:**
- Dataset generation Ú©Û’ Ù„ÛŒÛ’ Replicator script
- Training script Ø§ÙˆØ± logs
- Trained model weights
- Ù…ÛŒÙ¹Ø±Ú©Ø³ Ú©Û’ Ø³Ø§ØªÚ¾ evaluation Ø±Ù¾ÙˆØ±Ù¹

**Ø¨ÙˆÙ†Ø³ (+20 Ù¾ÙˆØ§Ø¦Ù†Ù¹Ø³):**
- Custom domain randomization Ù†Ø§ÙØ° Ú©Ø±ÛŒÚº (distractor objectsØŒ camera noise)
- Ø³Ø§Ø¯Û manipulation task Ú©Û’ Ù„ÛŒÛ’ RL policy Ù¹Ø±ÛŒÙ†Ù†Ú¯ Ú©Ø±ÛŒÚº

## ÙˆØ³Ø§Ø¦Ù„

- [Replicator Documentation](https://docs.omniverse.nvidia.com/extensions/latest/ext_replicator.html)
- [Isaac Gym Documentation](https://docs.omniverse.nvidia.com/isaacsim/latest/isaac_gym_tutorials/index.html)
- [Synthetic Data Generation Guide](https://docs.omniverse.nvidia.com/isaacsim/latest/replicator_tutorials/index.html)
- [Domain Randomization Paper](https://arxiv.org/abs/1703.06907)
- [Isaac Gym Benchmark](https://leggedrobotics.github.io/rl-games/)

## Ø§Ú¯Ù„Û’ Ù‚Ø¯Ù…

Ø¨ÛØªØ±ÛŒÙ† Ú©Ø§Ù…! Ø§Ø¨ Ø¢Ù¾ synthetic data generation Ø§ÙˆØ± parallel RL training Ø³Ù…Ø¬Ú¾ØªÛ’ ÛÛŒÚºÛ”

Ø§Ú¯Ù„Ø§ ÛÙØªÛ: [ÛÙØªÛ 10: Sim-to-Real Transfer & Chapter 3 Project](week-10.md)

ÛÙ… sim-to-real gap Ø³Û’ Ù†Ù…Ù¹ÛŒÚº Ú¯Û’ Ø§ÙˆØ± Ø§ÛŒÚ© Ø¬Ø§Ù…Ø¹ Isaac Sim Ù¾Ø±ÙˆØ¬ÛŒÚ©Ù¹ Ù…Ú©Ù…Ù„ Ú©Ø±ÛŒÚº Ú¯Û’!

---

## ğŸ“ ÛÙØªÛ ÙˆØ§Ø± Quiz

Ø§Ø³ ÛÙØªÛ’ Ú©Û’ Ù…ÙˆØ§Ø¯ Ú©ÛŒ Ø§Ù¾Ù†ÛŒ Ø³Ù…Ø¬Ú¾ Ú©Ùˆ Ø¬Ø§Ù†Ú†ÛŒÚº! Quiz multiple choice ÛÛ’ØŒ Ø®ÙˆØ¯Ú©Ø§Ø± Ø·ÙˆØ± Ù¾Ø± score ÛÙˆØªØ§ ÛÛ’ØŒ Ø§ÙˆØ± Ø¢Ù¾ Ú©Û’ Ù¾Ø§Ø³ 2 Ú©ÙˆØ´Ø´ÛŒÚº ÛÛŒÚºÛ”

**[ÛÙØªÛ 9 Quiz Ù„ÛŒÚº â†’](/quiz?week=9)**
